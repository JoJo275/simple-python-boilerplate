# Learning Notes

Personal notes and learnings captured while building this project.

## Python Packaging

### The `src/` Layout Mystery (Solved)

**Problem:** Tests fail with `ModuleNotFoundError` even though code is right there.

**Why:** Python doesn't automatically look inside `src/`. The `src/` layout is *intentionally* strict â€” it forces you to install the package properly.

**Solution:** Always run `pip install -e .` after cloning. The `-e` (editable) flag links your source so changes reflect immediately.

### pyproject.toml vs setup.py

- `setup.py` = Old way (executable Python, security concerns)
- `pyproject.toml` = New way (declarative TOML, standard)

Most tools now read from `[tool.X]` sections in pyproject.toml. One file to rule them all.

### Hatchling vs Hatch â€” The Mental Model

These two names kept confusing me. The key distinction:

- **Hatchling** = "how to build the package" (build backend, like setuptools)
- **Hatch** = "how to work on the project" (project manager, like tox/nox + venv)

| Tool | What it does | Config section |
|------|-------------|----------------|
| **Hatchling** | Builds sdist/wheel when you `pip install .` or `python -m build` | `[build-system]`, `[tool.hatch.build.*]` |
| **Hatch** | Manages envs, runs scripts, bumps versions, triggers builds | `[tool.hatch.envs.*]`, `[tool.hatch.version]` |

**Why use both?** A single `pyproject.toml` defines everything:
- Build backend (Hatchling)
- What goes into distributions (include/exclude rules)
- Version source/bumping rules
- Dev/test/lint environments and scripts (Hatch)

**Important:** Hatchling works *without* Hatch installed. Anyone can `pip install .` and Hatchling handles the build. Hatch is optional â€” it's a convenience CLI for developers.

See: [ADR 016](../adr/016-hatchling-and-hatch.md)

### Lockfiles and Transitive Dependencies â€” pip-tools vs uv vs Poetry

**The problem:** When you declare `requests>=2.28` in `pyproject.toml`, pip resolves it to *some* version at install time, along with transitive deps like `urllib3`, `certifi`, etc. Different machines at different times can end up with different versions â€” "works on my machine" bugs.

**The solution:** A lockfile pins *every* dependency (direct + transitive) to exact versions+hashes, ensuring reproducible environments.

#### The Three Main Approaches

| Tool | Lockfile | How it works | Best for |
|------|----------|--------------|----------|
| **pip-tools** | `requirements.txt` (generated) | `pip-compile` reads your loose deps and outputs pinned `requirements.txt`. `pip-sync` installs exactly what's in the file. | Existing pip-based workflows, minimal learning curve |
| **uv** | `uv.lock` | Rust-based, drop-in pip replacement. `uv lock` generates lockfile, `uv sync` installs from it. 10-100x faster than pip. | Speed-critical workflows, modern replacement for pip+venv |
| **Poetry** | `poetry.lock` | Full project manager. `poetry install` reads `pyproject.toml`, generates/uses `poetry.lock`. Also handles builds and publishing. | All-in-one solution, teams wanting integrated tooling |

#### How They Compare

| Aspect | pip-tools | uv | Poetry |
|--------|-----------|-----|--------|
| **Speed** | Slow (pip resolver) | Blazing fast (Rust) | Moderate |
| **Config file** | `pyproject.toml` or `requirements.in` | `pyproject.toml` | `pyproject.toml` (but `[tool.poetry]`, not PEP 621) |
| **Learning curve** | Minimal â€” familiar pip workflow | Low â€” pip-compatible commands | Moderate â€” own CLI and concepts |
| **Hatch compatibility** | Works alongside (but awkward) | Can replace Hatch entirely | Replaces Hatch (different philosophy) |
| **Maturity** | Very mature | Newer but rapidly adopted | Mature, large community |

#### Why This Project Uses Hatch (Without Lockfiles)

Hatch doesn't have native lockfile support â€” it re-resolves dependencies on each `hatch env create`. This is fine for:

- **Template projects** â€” users will replace deps anyway
- **Libraries** â€” consumers control their own dep versions
- **Development** â€” fast iteration matters more than reproducibility

If you need lockfiles for **deployed applications** where reproducibility is critical, consider:

1. **Switch to uv** â€” fastest, modern, has `uv.lock`
2. **Use Poetry** â€” mature, well-documented, has `poetry.lock`
3. **Bolt pip-tools onto Hatch** â€” possible but awkward (Hatch fights you)

The hybrid approach (Hatch for dev, pip-compile for deploy) adds complexity â€” usually cleaner to pick one tool that handles both.

#### pip-tools Workflow (If You Did Use It)

```bash
# 1. Create requirements.in with loose constraints
echo "requests>=2.28" > requirements.in

# 2. Compile to pinned requirements.txt
pip-compile requirements.in --output-file=requirements.txt

# 3. Install exactly those versions
pip-sync requirements.txt

# 4. Update when needed
pip-compile --upgrade requirements.in
```

The lockfile becomes the source of truth â€” commit it, use it in CI and production.

### Python Tool Landscape

There are a lot of overlapping tools in the Python ecosystem. This table groups them by purpose.

#### Build Backends (what `pip install .` uses)

| Tool | Description | Capabilities | Pros | Cons |
|------|-------------|-------------|------|------|
| **Hatchling** | Modern build backend by the Hatch project | Build sdist/wheel, auto-discover packages, include/exclude rules, plugins | Fast, minimal config, auto-discovers src/ layout | Newer, smaller ecosystem than setuptools |
| **setuptools** | The original build backend | Build sdist/wheel, C extensions, entry points, data files, find_packages | Ubiquitous, massive community, battle-tested | Verbose config, legacy baggage (`setup.py`, `setup.cfg`) |
| **Flit-core** | Minimalist build backend | Build sdist/wheel for pure-Python packages | Dead simple for pure-Python packages | No compiled extensions, fewer features |
| **PDM-backend** | Build backend from the PDM project | Build sdist/wheel, PEP 621 metadata, editable installs | PEP 621 native, supports lock files | Tied to PDM ecosystem |
| **Maturin** | Build backend for Rust+Python (PyO3) | Build wheels with compiled Rust extensions, cross-compile | First-class Rust FFI support | Only for Rust extensions |

#### Project/Environment Managers (create envs, run tasks)

| Tool | Description | Capabilities | Pros | Cons |
|------|-------------|-------------|------|------|
| **Hatch** | Project manager + env manager | Create/manage envs, run scripts, test matrices, version bumping, build, publish | Env management, test matrices, scripts, version bumping â€” all in `pyproject.toml` | Less established than tox in older codebases |
| **tox** | Test automation / environment manager | Multi-env test runs, dependency isolation, CI integration, plugin system | Very mature, widely adopted in CI, plugin ecosystem | Separate `tox.ini` config, can be verbose |
| **nox** | Like tox but config is Python code | Session-based test runs, parametrize, reuse venvs, conda support | Full Python flexibility, easy to debug sessions | Requires writing Python (pro or con), no declarative config |
| **PDM** | Package + project manager | Dependency resolution, lock files, scripts, env management, publish | PEP 582 support, lock files, scripts | Different philosophy (centralised tool), smaller community |
| **uv** | Fast package installer + env manager (Rust) | Install packages, create venvs, resolve dependencies, run scripts | Extremely fast, drop-in pip/venv replacement | Newer tool, still evolving rapidly |
| **Poetry** | Dependency manager + build tool | Dependency resolution, lock files, env management, build, publish | Lock files, dependency resolution, publish built in | Own config format (`[tool.poetry]`), doesn't follow PEP 621 fully |
| **Pipenv** | pip + virtualenv wrapper | Pipfile/Pipfile.lock, auto-create venvs, `.env` loading | Lock files, `.env` support | Slow dependency resolution, less active development |

#### Task Runners (run commands/scripts)

| Tool | Description | Capabilities | Pros | Cons |
|------|-------------|-------------|------|------|
| **Hatch scripts** | Scripts defined in `pyproject.toml` | Run commands, chain scripts, pass args, env-aware | Zero extra tools, integrated with Hatch envs | Only available through Hatch |
| **Make** | Classic build automation (Makefile) | Targets, dependencies, variables, shell commands, parallel builds | Universal, available everywhere, well understood | Not Python-native, Windows requires extra setup, tab-sensitive syntax |
| **just** | Modern command runner (Justfile) | Recipes, arguments, variables, dotenv loading, cross-platform | Simple syntax, cross-platform, no tab issues | Extra binary to install, not Python-specific |
| **Task (go-task)** | Task runner using `Taskfile.yml` | Tasks, dependencies, variables, watch mode, cross-platform | YAML-based, cross-platform, dependency graphs | Extra binary, Go ecosystem tool |
| **invoke** | Python-based task runner | Tasks as Python functions, namespaces, auto-parsing args | Pure Python, good for complex logic | Another dependency, less popular now |
| **nox** | Also works as a task runner | Session-based commands, parametrize, venv per session | Python-based, session isolation | Heavier than a simple task runner |
| **tox** | Also works as a task runner | Env-isolated command runs, dependency pinning | Mature, env isolation | Verbose for simple tasks |

#### CLI Frameworks (building user-facing CLIs)

| Tool | Description | Capabilities | Pros | Cons |
|------|-------------|-------------|------|------|
| **argparse** | Standard library CLI parser | Positional/optional args, subcommands, type conversion, help generation | No dependencies, always available | Verbose, manual help formatting |
| **click** | Decorator-based CLI framework | Commands, groups, options, prompts, file handling, colour output, plugins | Clean API, composable commands, excellent docs | Extra dependency |
| **typer** | Click-based, uses type hints for CLI args | Auto CLI from type hints, auto-completion, rich help | Minimal boilerplate, auto-generates help | Depends on click, newer |
| **rich-click** | Click + Rich for beautiful help output | Rich-formatted help, panels, syntax highlighting, tables | Pretty terminal output, drop-in for click | Extra dependency on top of click |
| **fire** | Auto-generates CLI from any Python object | Auto CLI from functions/classes/objects, no decorators needed | Zero boilerplate | Less control over help text and validation |

#### Linting & Formatting

| Tool | Description | Capabilities | Pros | Cons |
|------|-------------|-------------|------|------|
| **Ruff** | Linter + formatter (Rust) | 800+ lint rules, auto-fix, import sorting, formatting, pyupgrade | Blazing fast, replaces flake8+isort+black+pyupgrade+more | Newer, not 100% rule parity with all tools |
| **flake8** | Linter | Style checks, error detection, plugin system (200+ plugins) | Mature, huge plugin ecosystem | Slower, Python-based, being superseded by Ruff |
| **Black** | Opinionated formatter | Deterministic formatting, magic trailing comma, string normalisation | Zero config, consistent | No flexibility, being superseded by Ruff |
| **isort** | Import sorter | Sort imports, configurable sections, profiles (black-compatible) | Focused, configurable | Separate tool, Ruff handles this now |
| **autopep8** | PEP 8 formatter | Fix PEP 8 violations, conservative by default | Conservative formatting | Less opinionated than Black, less popular |

#### Type Checkers

| Tool | Description | Capabilities | Pros | Cons |
|------|-------------|-------------|------|------|
| **mypy** | The original Python type checker | Strict mode, incremental checks, stubs, plugins, daemon mode | Most mature, wide adoption, plugin ecosystem | Slower, can be strict to configure |
| **Pyright** | Type checker (powers VS Code Pylance) | Full type inference, watch mode, multi-root workspaces, strict mode | Fast, excellent IDE integration | Node.js dependency, different strictness defaults |
| **pytype** | Google's type checker | Type inference without annotations, cross-function analysis | Infers types even without annotations | Less widely used outside Google |

#### Dependency & Security Tools

| Tool | Install | Capabilities | Pros | Cons |
|------|---------|-------------|------|------|
| **pip-tools** | `pip install pip-tools` | `pip-compile` pins deps to a lock file, `pip-sync` installs exactly those pins | Reproducible builds, minimal lock file, works with pip | Extra step in workflow, no auto-update |
| **pip-audit** | `pip install pip-audit` | Scan installed packages against known vulnerability databases (OSV, PyPI) | Fast, integrates with CI, supports requirements.txt and pyproject.toml | Only checks known CVEs, not code-level issues |
| **pipdeptree** | `pip install pipdeptree` | Visualise dependency tree, detect conflicts, show reverse deps | Great for debugging dependency issues, simple output | Read-only â€” doesn't fix problems |

#### Debugging & Developer Experience Tools

| Tool | Install | Capabilities | Pros | Cons |
|------|---------|-------------|------|------|
| **rich** | `pip install rich` | Pretty tables, tracebacks, progress bars, syntax highlighting, logging, markdown rendering | Beautiful console output, drop-in traceback handler | Extra dependency, large package |
| **icecream** | `pip install icecream` | `ic(variable)` â€” prints variable name + value + file/line, auto-formats | Much better than `print()` debugging, zero-config | Debug-only â€” must remove before committing |
| **ipython** | `pip install ipython` | Enhanced REPL with tab completion, syntax highlighting, magic commands, `%timeit`, `%debug` | Far better than default Python shell, auto-reload modules | Heavier dependency, not for production |
| **devtools** | `pip install devtools` | `debug(variable)` â€” pretty-prints with type info, file/line, colour output | Clean debug output, type-aware formatting | Less known than icecream, similar purpose |

#### Commit Convention & Versioning Tools

| Tool | Install | Capabilities | Pros | Cons |
|------|---------|-------------|------|------|
| **commitizen** | `pip install commitizen` | Interactive commit prompts enforcing Conventional Commits, auto-bump version, auto-generate changelog, pre-commit hook, CI validation | All-in-one: commit format + version bump + changelog, configurable via `pyproject.toml`, supports custom commit schemas | Python dependency, learning curve for custom rules |
| **commitlint** | `npm install @commitlint/cli` | Lint commit messages against Conventional Commits (or custom) rules, integrates with husky | Huge ecosystem, very configurable rules | Node.js dependency, doesn't bump versions or generate changelogs |
| **semantic-release** | `pip install python-semantic-release` | Auto-determine next version from commits, generate changelog, create Git tags, publish to PyPI | Fully automated release pipeline, CI-friendly | Opinionated workflow, less control over individual steps |
| **towncrier** | `pip install towncrier` | Fragment-based changelog generation â€” each PR adds a news fragment file, assembled at release | Avoids merge conflicts in CHANGELOG, per-PR granularity | Extra workflow step (create fragment file per change), not commit-based |
| **standard-version** | `npm install standard-version` | Bump version, generate changelog from Conventional Commits, create Git tag | Simple, focused on versioning + changelog | Node.js dependency, archived/maintenance-only |
| **bump2version** | `pip install bump2version` | Find-and-replace version strings across files, create Git tag | Simple, language-agnostic, config file driven | No commit message parsing, no changelog generation, maintenance mode |

#### Commit Message Prefixes (Conventional Commits)

| Prefix | Meaning | When to use | Version bump | Example |
|--------|---------|-------------|--------------|---------|
| `feat:` | New feature | Adding new user-facing functionality | Minor | `feat: add user login endpoint` |
| `fix:` | Bug fix | Fixing a defect or incorrect behavior | Patch | `fix: correct null check in parser` |
| `docs:` | Documentation | Changes to documentation only | None | `docs: update API usage guide` |
| `style:` | Code style | Formatting, whitespace â€” no logic change | None | `style: fix indentation in models` |
| `refactor:` | Refactoring | Restructuring code without changing behavior | None | `refactor: extract validation into helper` |
| `perf:` | Performance | Improving performance without changing behavior | Patch | `perf: cache database query results` |
| `test:` | Tests | Adding or updating tests only | None | `test: add unit tests for auth module` |
| `build:` | Build system | Changes to build config or dependencies | None | `build: upgrade hatchling to 1.25` |
| `ci:` | CI/CD | Changes to CI/CD configuration or scripts | None | `ci: add mypy check to PR workflow` |
| `chore:` | Maintenance | Routine tasks, tooling, no production code change | None | `chore: update .gitignore` |
| `revert:` | Revert | Reverting a previous commit | Varies | `revert: undo feat: add login endpoint` |
| `feat!:` / `fix!:` | Breaking change | Append `!` after any type for incompatible API changes | Major | `feat!: remove deprecated login endpoint` |

> **Tip:** Use a `BREAKING CHANGE:` footer in the commit body for longer breaking change explanations.

#### Branch Prefixes

| Branch prefix | Meaning | When to use | Typical merge expectation | Example |
|---------------|---------|-------------|---------------------------|---------|
| `wip/` | Work in progress | Incomplete work, not ready for review | Draft PR or no PR yet | `wip/user-auth-flow` |
| `spike/` | Technical spike | Time-boxed research or proof of concept | May not merge â€” results documented | `spike/graphql-feasibility` |
| `explore/` | Exploration | Experimenting with an idea or library | May not merge â€” learning exercise | `explore/htmx-integration` |
| `chore/` | Maintenance | Routine tasks, config, tooling changes | Merge after review | `chore/update-gitignore` |
| `feat/` | Feature | New user-facing functionality | Merge after review + tests pass | `feat/user-login` |
| `fix/` | Bug fix | Fixing a defect or incorrect behavior | Merge after review + tests pass | `fix/null-pointer-in-parser` |
| `docs/` | Documentation | Changes to documentation only | Merge after review | `docs/update-readme` |
| `refactor/` | Refactoring | Restructuring code without changing behavior | Merge after review + tests pass | `refactor/extract-auth-service` |
| `test/` | Tests | Adding or updating tests only | Merge after review | `test/add-auth-unit-tests` |
| `ci/` | CI/CD | Changes to CI/CD configuration | Merge after review | `ci/add-mypy-workflow` |
| `build/` | Build system | Build config, packaging, dependencies | Merge after review | `build/upgrade-hatchling` |
| `perf/` | Performance | Performance improvements | Merge after review + benchmarks | `perf/cache-db-queries` |
| `style/` | Code style | Formatting, whitespace â€” no logic change | Merge after review | `style/fix-indentation` |
| `release/` | Release | Preparing a release (version bump, changelog) | Merge to main, tag, deploy | `release/v1.2.0` |
| `hotfix/` | Hotfix | Urgent production fix | Fast-track merge + deploy | `hotfix/fix-login-crash` |
| `deps/` | Dependencies | Dependency updates (manual or grouped) | Merge after CI passes | `deps/bump-requests-2.32` |
| `sec/` | Security | Security-related fix or hardening | Merge after review (may be private) | `sec/patch-ssrf-vulnerability` |

#### What this project uses

| Category | Tool | Why |
|----------|------|-----|
| Build backend | Hatchling | Auto-discovers src/ layout, minimal config |
| Project manager | Hatch | Envs, scripts, test matrices â€” one `pyproject.toml` |
| Task runner | Hatch scripts | No extra tools needed |
| CLI framework | argparse | No dependencies for a simple boilerplate |
| Linter + formatter | Ruff | Fast, replaces multiple tools |
| Type checker | mypy | Most mature, strict mode |
| Testing | pytest | De facto standard |

---

## GitHub Actions

### Why Pin to SHAs?

Tags like `@v4` are mutable â€” someone could push malicious code and move the tag. SHAs are immutable. Always pin to full SHA with a version comment:

```yaml
uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
```

### Workflow Organization

Separate files > one giant file:
- Easier to disable (just rename to `_workflow.yml`)
- Each gets its own permissions
- Failures are isolated

### Secrets vs Variables

GitHub Actions has two ways to store configuration values:

| Aspect | Secrets | Variables |
|--------|---------|-----------|
| **Visibility** | Hidden forever after creation | Visible to anyone with repo access |
| **In logs** | Auto-masked if printed (`***`) | Printed in plain text |
| **Storage** | Encrypted at rest | Plain text |
| **Access** | `${{ secrets.NAME }}` | `${{ vars.NAME }}` |
| **Use for** | Tokens, passwords, API keys | Feature flags, non-sensitive config |

**Rule of thumb:** If it's a token, key, or credential â€” use **Secrets**. If it's a
simple on/off flag or display value â€” use **Variables**.

**Example uses in this project:**

| Value | Type | Why |
|-------|------|-----|
| `CODECOV_TOKEN` | Secret | Authenticates coverage uploads â€” leak = account compromise |
| `ENABLE_WORKFLOWS` | Variable | Just a feature flag â€” no security impact |
| `NPM_TOKEN` | Secret | Publish access to npm registry |

**Setting them:**

1. Repo â†’ **Settings** â†’ **Secrets and variables** â†’ **Actions**
2. Click **Secrets** tab or **Variables** tab
3. Click **New repository secret** or **New repository variable**

---

## Static Analysis Tools

| Tool | Purpose | Speed |
|------|---------|-------|
| **Ruff** | Linting + formatting | âš¡ Very fast (Rust) |
| **Mypy** | Type checking | ğŸ¢ Slower |
| **Pyright** | Type checking (VS Code) | âš¡ Fast |
| **Bandit** | Security scanning | ğŸ¢ Moderate |

**Ruff** replaces: flake8, isort, black, pyupgrade, and more. One tool, one config.

---

## Quality Gates

A **quality gate** is a checkpoint that code must pass before moving forward (e.g., merging to main, deploying to production).

### Common Quality Gates in CI

| Gate | What It Checks | Tool |
|------|----------------|------|
| **Tests pass** | Code works as expected | pytest |
| **Linting passes** | Code style, bugs | Ruff |
| **Type checking passes** | Type correctness | Mypy/Pyright |
| **Coverage threshold** | Enough tests exist | pytest-cov |
| **Security scan** | No vulnerabilities | Bandit, pip-audit |
| **Spell check** | No typos | codespell |

> **codespell: report-only by default.** codespell does *not* auto-fix typos unless you pass `--write-changes` (or `-w`). Without that flag it reports the misspelling and a suggested fix, then exits non-zero â€” which blocks your commit. You have two options:
>
> 1. **Auto-fix:** Add `-w` to the hook args in `.pre-commit-config.yaml`:
>    ```yaml
>    - id: codespell
>      args: [-w, --skip, ".git,.venv,dist,build,..."]
>    ```
>    codespell will rewrite the file in-place. The commit still fails (the file changed), but re-running `git add` + `git commit` picks up the fix.
>
> 2. **Manual fix:** Read the output, fix the typo yourself, then re-commit. This is safer when codespell's suggestion is wrong (it happens with domain-specific terms).
>
> **Tip:** If codespell flags a word that's correct (e.g., a variable name or technical term), add it to an ignore list: `args: [-L, "word1,word2", --skip, "..."]` or set `[tool.codespell]` in `pyproject.toml` with `ignore-words-list = "word1,word2"`.

### Enforcing Quality Gates

1. **GitHub Branch Protection** â€” Require status checks to pass before merge
2. **CI Workflow** â€” Each job is a gate; if one fails, PR can't merge
3. **Pre-commit Hooks** â€” Catch issues before they even reach CI

### Soft vs Hard Gates

- **Hard gate** â€” Must pass (blocks merge/deploy)
- **Soft gate** â€” Informational only (warns but doesn't block)

Example: When adopting type checking, start with a soft gate (`continue-on-error: true`) while adding type hints gradually.

### Why Quality Gates Matter

- Catch bugs early (cheaper to fix)
- Maintain code consistency
- Build confidence in deployments
- Document quality expectations

---

## Containers â€” Production vs Development vs Orchestration

This project has three container-related files that serve completely different purposes.
Understanding the distinction is important.

### The Big Picture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Docker / Podman       â”‚  â† The engine that runs everything
                    â”‚   (container runtime)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                   â”‚                   â”‚
            â–¼                   â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Containerfile â”‚   â”‚ devcontainer  â”‚   â”‚ docker-composeâ”‚
    â”‚ (production)  â”‚   â”‚ (development) â”‚   â”‚ (orchestration)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparison Table

| Aspect | Containerfile | Dev Container | Docker Compose |
|--------|---------------|---------------|----------------|
| **Location** | `Containerfile` (repo root) | `.devcontainer/` | `docker-compose.yml` |
| **Purpose** | Build production image | Development environment | Run/orchestrate containers |
| **Contains** | Minimal app only (~150MB) | Full dev tools (~1GB+) | References other images |
| **User** | Run the application | Write code interactively | Manage multi-service setups |
| **When to use** | CI/CD, deployment | Daily development | Local testing, multi-container |

### 1. Containerfile (Production)

A recipe for building a minimal **production** container image. Contains only your
installed application â€” no dev tools, no tests, no source code.

**How to use:**
```bash
# Build the image
docker build -t simple-python-boilerplate -f Containerfile .

# Run your application
docker run --rm simple-python-boilerplate
```

**Key features:**
- Multi-stage build (builder stage + runtime stage)
- Non-root user for security
- Pinned base image digest for reproducibility
- OCI-compliant (works with Docker, Podman, etc.)

### 2. Dev Container (Development)

A VS Code feature that runs your entire development environment inside a container.
Everything is pre-configured: Python, Node.js, pre-commit hooks, extensions.

**How to use:**
1. Install Docker Desktop (or Podman)
2. Install VS Code extension: "Dev Containers"
3. Open repo in VS Code
4. Click "Reopen in Container" (or Cmd/Ctrl+Shift+P â†’ `Dev Containers: Reopen in Container`)

Alternatively, use GitHub Codespaces â€” the config works there automatically.

**Key features:**
- Zero-setup onboarding for new contributors
- Consistent environment across machines
- All extensions and settings pre-configured
- Works with GitHub Codespaces

### 3. Docker Compose (Orchestration)

A declarative way to build/run containers with all options specified in a file.
Useful for local testing and multi-service setups (app + database, etc.).

**How to use:**
```bash
# Build and run
docker compose up --build

# Run in background
docker compose up -d --build

# Stop
docker compose down

# View logs
docker compose logs -f
```

**Key features:**
- Version-controlled run configuration
- Easy to add services (database, cache, etc.)
- Simpler than remembering `docker run` flags

### When to Use Which

| Scenario | Use |
|----------|-----|
| "I want to deploy this app" | Containerfile â†’ build image â†’ push to registry |
| "I want to develop this project" | Dev Container â†’ VS Code "Reopen in Container" |
| "I want to test the production build locally" | Docker Compose â†’ `docker compose up --build` |
| "I want to run app + database together" | Docker Compose with multiple services |
| "I'm a new contributor, how do I start?" | Dev Container or Codespaces |

### Docker vs Podman

Both are container runtimes that can execute all three configurations above:

| Aspect | Docker | Podman |
|--------|--------|--------|
| **Architecture** | Client-server (daemon) | Daemonless |
| **Root required** | Historically yes | Rootless by default |
| **Compatibility** | Industry standard | Docker CLI-compatible |
| **License** | Docker Desktop requires license for enterprises | Fully open source |

For most purposes, they're interchangeable. This project uses "Containerfile" (Podman's
preferred name) instead of "Dockerfile" but both tools understand both names.

See: [ADR 025](../adr/025-container-strategy.md)

---

## Virtual Environments

### Quick Setup

```bash
python -m venv .venv
.venv\Scripts\Activate.ps1  # Windows
source .venv/bin/activate   # macOS/Linux
pip install -e ".[dev]"
```

### Check Which Python

```bash
python -c "import sys; print(sys.executable)"
```

If it doesn't show `.venv`, you're using the wrong Python!

### Viewing Installed Packages

```bash
# List all packages in the current environment (venv or global)
pip list

# Same but in requirements.txt format (useful for diffing)
pip freeze

# Show details for a specific package (version, location, dependencies)
pip show <package-name>

# Show where pip is installing to
pip -V
```

**Global vs local (venv):** The commands above always operate on whichever Python environment is active. If a venv is activated, they show/affect only that venv. If no venv is active, they target the global (user or system) Python.

To explicitly target the global Python when a venv is active:

```bash
# Windows â€” use the full path to the global pip
& "C:\Users\$env:USERNAME\AppData\Local\Programs\Python\Python3*\Scripts\pip.exe" list

# macOS/Linux
/usr/bin/python3 -m pip list
# or
python3 -m pip list  # if no venv is active
```

### Removing Packages

```bash
# Uninstall a single package
pip uninstall <package-name>

# Uninstall without confirmation prompt
pip uninstall -y <package-name>
```

#### Bulk-Remove All pip Packages

To wipe every package in the current environment (useful for a clean slate):

```bash
# Generate the list and pipe it to uninstall (keeps pip itself)
pip freeze | xargs pip uninstall -y

# PowerShell equivalent
pip freeze | ForEach-Object { pip uninstall -y $_ }
```

> **Easier alternative for venvs:** Just delete and recreate the venv. It's faster and guarantees a clean state:
>
> ```bash
> deactivate                    # exit the venv first
> Remove-Item -Recurse .venv   # Windows PowerShell
> rm -rf .venv                 # macOS/Linux
> python -m venv .venv         # recreate
> pip install -e ".[dev]"      # reinstall project deps
> ```

#### Remove a Global Package

```bash
# Deactivate any venv first, then uninstall
deactivate
pip uninstall <package-name>
```

> **Tip:** Avoid installing packages globally. Use virtual environments for project work and `pipx` for standalone CLI tools (e.g., `pipx install ruff`). This keeps the global Python clean and avoids version conflicts.

---

## Command Workflow â€” How Tools Layer Together

Understanding how commands flow through the tooling stack helps when debugging
issues or customizing workflows.

### The Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOU (developer)                                                            â”‚
â”‚  â†“                                                                          â”‚
â”‚  task test         â† Task runner (Taskfile.yml) â€” human-friendly wrapper    â”‚
â”‚  â†“                                                                          â”‚
â”‚  hatch run test    â† Hatch â€” manages virtualenv + runs command inside it    â”‚
â”‚  â†“                                                                          â”‚
â”‚  pytest            â† Actual tool â€” runs in the Hatch-managed environment    â”‚
â”‚  â†“                                                                          â”‚
â”‚  Python            â† Interpreter â€” executes the test code                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Three Ways to Run the Same Thing

| Command | What happens |
|---------|--------------|
| `task test` | Taskfile finds `test:` task â†’ runs `hatch run test` |
| `hatch run test` | Hatch activates `default` env â†’ runs `pytest` |
| `pytest` | Direct call â€” only works if you're already in a venv with deps installed |

All three ultimately run pytest. The layers add convenience:

- **Task** â€” Memorable names, can chain commands, no Hatch knowledge needed
- **Hatch** â€” Ensures correct virtualenv, handles Python version matrix
- **Direct** â€” Fast, but requires manual env management

### Where Each Layer Is Configured

| Layer | Config file | What it defines |
|-------|-------------|-----------------|
| **Task** | `Taskfile.yml` | Task names, descriptions, which `hatch run` commands to call |
| **Hatch envs** | `pyproject.toml` `[tool.hatch.envs.*]` | Environment names, features, Python versions |
| **Hatch scripts** | `pyproject.toml` `[tool.hatch.envs.*.scripts]` | Script names â†’ actual CLI commands |
| **Tools** | `pyproject.toml` `[tool.*]` | Tool-specific config (pytest, ruff, mypy, coverage) |

### Example: Tracing `task lint`

```
task lint
  â””â†’ Taskfile.yml defines: cmds: ["hatch run lint"]
      â””â†’ pyproject.toml [tool.hatch.envs.default.scripts] defines: lint = "ruff check src/ tests/"
          â””â†’ ruff check src/ tests/
              â””â†’ ruff reads [tool.ruff] from pyproject.toml
```

### When CI Skips Taskfile

GitHub Actions workflows call `hatch run` directly, not `task`. Why?

1. **Fewer dependencies** â€” No need to install Task binary in CI
2. **Explicit** â€” YAML shows exactly what runs, no indirection
3. **Standard** â€” Other projects can copy workflow without Taskfile adoption

```yaml
# CI workflow
- run: hatch run test    # Direct, not `task test`
```

### Direct Execution (Skip All Layers)

If you're in the venv already, you can call tools directly:

```bash
# After: hatch shell  OR  source .venv/bin/activate
pytest                  # No hatch/task wrapper
ruff check src/
mypy src/
```

This is faster for quick checks but bypasses Hatch's environment guarantees.

### Debugging Tips

| Problem | Check |
|---------|-------|
| "Command not found" | Are you in a venv? Run `hatch shell` or activate manually |
| "Task not found" | Is Task installed? `task --version` or use `hatch run` directly |
| "hatch run X fails" | Does the script exist in `[tool.hatch.envs.default.scripts]`? |
| "Works locally, fails in CI" | CI uses `hatch run`, not `task`. Check if they match. |

### Why Not Just Use Make?

This project uses Taskfile instead of Make because:

- **Cross-platform** â€” Works identically on Windows, no `make` installation needed
- **YAML > Makefile syntax** â€” No tab-sensitivity issues
- **Built-in help** â€” `task` lists all tasks with descriptions

See: [ADR 017](../adr/017-task-runner.md)

---

## Pre-commit Hooks

Pre-commit hooks run checks *before* code is committed, catching issues locally before they reach CI.

### Setup

```bash
pip install pre-commit
pre-commit install
```

### Configuration (`.pre-commit-config.yaml`)

```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.8.6
    hooks:
      - id: ruff          # Linting
        args: [--fix]
      - id: ruff-format   # Formatting

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.14.1
    hooks:
      - id: mypy
        additional_dependencies: []

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
```

### Key Commands

| Command | Purpose |
|---------|---------|
| `pre-commit install` | Enable hooks for this repo |
| `pre-commit run --all-files` | Run on all files (not just staged) |
| `pre-commit autoupdate` | Update hook versions |
| `git commit --no-verify` | Skip hooks (emergency only!) |

### Why Pre-commit > Manual Checks

- **Automatic** â€” Can't forget to run it
- **Fast feedback** â€” Fix before pushing
- **Consistent** â€” Same checks for everyone
- **CI friendly** â€” Run same hooks in CI as backup

### Authoring Custom Git Hooks

Beyond using existing hooks, you can author your own and publish them for others to use. Custom hooks live in a Git repository with a `.pre-commit-hooks.yaml` file at the root that declares the available hooks.

#### How It Works

1. Create a new Git repository for your hook(s).
2. Write the script or tool that performs the check.
3. Add a `.pre-commit-hooks.yaml` file describing the hook(s).
4. Tag a release â€” consumers pin to this tag via `rev` in their `.pre-commit-config.yaml`.

#### `.pre-commit-hooks.yaml` Fields

The `.pre-commit-hooks.yaml` file is a list of hook definitions. Each entry supports these fields:

| Field | Required | Description |
|-------|----------|-------------|
| `id` | Yes | Unique identifier for the hook (used in consumers' `hooks:` list) |
| `name` | Yes | Human-readable name shown during execution |
| `entry` | Yes | The executable to run (script path, command, or console_script) |
| `language` | Yes | How to install/run the hook (`python`, `node`, `system`, `script`, `docker`, etc.) |
| `files` | No | Regex pattern for filenames to pass to the hook (default: `''` â€” all files) |
| `exclude` | No | Regex pattern for filenames to exclude |
| `types` | No | File types to filter on (e.g., `[python]`, `[yaml]`) â€” uses `identify` library types |
| `types_or` | No | Like `types` but matches if *any* type matches (OR logic instead of AND) |
| `exclude_types` | No | File types to exclude |
| `always_run` | No | If `true`, run even when no matching files are staged (default: `false`) |
| `pass_filenames` | No | If `true`, staged filenames are passed as arguments (default: `true`) |
| `require_serial` | No | If `true`, disable parallel execution for this hook (default: `false`) |
| `args` | No | Default arguments passed to `entry` (consumers can override via `args` in their config) |
| `description` | No | Short description of what the hook does |
| `minimum_pre_commit_version` | No | Minimum pre-commit version required (e.g., `'3.0.0'`) |
| `stages` | No | Which git hook stages to run in (e.g., `[pre-commit]`, `[pre-push]`, `[commit-msg]`) |
| `verbose` | No | If `true`, force hook output to be printed even on success (default: `false`) |
| `additional_dependencies` | No | Extra packages to install alongside the hook |
| `language_version` | No | Version of the language runtime to use (e.g., `python3.11`) |

#### Minimal Example

A simple hook that checks for `TODO` comments:

```yaml
# .pre-commit-hooks.yaml
- id: no-todos
  name: Check for TODO comments
  entry: grep -rn TODO
  language: system
  types: [python]
  pass_filenames: true
```

#### Python Script Hook Example

For hooks written in Python, structure the repo as an installable package:

```
my-hooks/
â”œâ”€â”€ .pre-commit-hooks.yaml
â”œâ”€â”€ pyproject.toml
â””â”€â”€ my_hooks/
    â””â”€â”€ check_something.py
```

```yaml
# .pre-commit-hooks.yaml
- id: check-something
  name: Check something custom
  entry: check-something       # console_scripts entry point
  language: python
  types: [python]
```

The `language: python` setting tells pre-commit to create an isolated virtualenv and `pip install` the hook repository, so any `console_scripts` defined in `pyproject.toml` become available as the `entry`.

#### Hook Stages

Git supports multiple hook points. Pre-commit can target different stages:

| Stage | Git Hook | When It Runs |
|-------|----------|--------------|
| `pre-commit` | `pre-commit` | Before commit is created (default) |
| `pre-merge-commit` | `pre-merge-commit` | Before merge commit is created |
| `pre-push` | `pre-push` | Before push to remote |
| `commit-msg` | `commit-msg` | After commit message is entered (can validate or modify it) |
| `post-checkout` | `post-checkout` | After `git checkout` or `git switch` |
| `post-commit` | `post-commit` | After commit is created |
| `post-merge` | `post-merge` | After a merge completes |
| `post-rewrite` | `post-rewrite` | After `git rebase` or `git commit --amend` |
| `prepare-commit-msg` | `prepare-commit-msg` | Before the commit message editor opens |
| `manual` | â€” | Only runs via `pre-commit run --hook-stage manual` |

To install hooks for non-default stages: `pre-commit install --hook-type commit-msg`

#### Common Hooks by Stage

> **Note:** The repos listed below are popular, widely-used choices â€” not an exhaustive list. Many alternative hooks exist for each stage. Browse [pre-commit.com/hooks](https://pre-commit.com/hooks.html) for a searchable directory.

**`pre-commit`** â€” Fast checks that run on every commit (the default stage):

| Hook | Repo | What It Does |
|------|------|--------------|
| `trailing-whitespace` | pre-commit/pre-commit-hooks | Strip trailing whitespace |
| `end-of-file-fixer` | pre-commit/pre-commit-hooks | Ensure files end with a newline |
| `check-yaml` | pre-commit/pre-commit-hooks | Validate YAML syntax |
| `check-toml` | pre-commit/pre-commit-hooks | Validate TOML syntax |
| `check-json` | pre-commit/pre-commit-hooks | Validate JSON syntax |
| `check-ast` | pre-commit/pre-commit-hooks | Validate Python syntax |
| `check-added-large-files` | pre-commit/pre-commit-hooks | Block oversized files |
| `check-merge-conflict` | pre-commit/pre-commit-hooks | Detect conflict markers (`<<<<<<<`) |
| `debug-statements` | pre-commit/pre-commit-hooks | Catch leftover `breakpoint()` / debugger imports |
| `detect-private-key` | pre-commit/pre-commit-hooks | Block private key files |
| `mixed-line-ending` | pre-commit/pre-commit-hooks | Normalize line endings |
| `ruff` | astral-sh/ruff-pre-commit | Lint Python (replaces flake8, isort, pyupgrade) |
| `ruff-format` | astral-sh/ruff-pre-commit | Format Python (replaces black) |
| `mypy` | pre-commit/mirrors-mypy | Type check Python |
| `bandit` | PyCQA/bandit | Security linting for Python |
| `codespell` | codespell-project/codespell | Catch common typos in code and docs |
| `validate-pyproject` | abravalheri/validate-pyproject | Validate pyproject.toml schema |
| `actionlint` | rhysd/actionlint | Lint GitHub Actions workflows |
| `check-github-workflows` | python-jsonschema/check-jsonschema | Validate workflow YAML against schema |

**`commit-msg`** â€” Validate or modify the commit message after the user writes it:

| Hook | Repo | What It Does |
|------|------|--------------|
| `conventional-pre-commit` | compilerla/conventional-pre-commit | Enforce Conventional Commits format |
| `commitizen` | commitizen-tools/commitizen | Validate commit message against commitizen rules |
| `commitlint` | alessandrojcm/commitlint-pre-commit-hook | Lint commit messages (Node-based) |
| `gitlint` | jorisroovers/gitlint | Configurable commit message linter |

**`pre-push`** â€” Slower checks that run before pushing to remote:

| Hook | Repo | What It Does |
|------|------|--------------|
| `pytest` (local) | local | Run full test suite |
| `gitleaks` | gitleaks/gitleaks | Secret detection across git history |
| `trufflehog` | trufflesecurity/trufflehog | Deep secret scanning |
| `mypy` | pre-commit/mirrors-mypy | Type check (if too slow for pre-commit) |
| `bandit` | PyCQA/bandit | Security scan (if too slow for pre-commit) |

**`prepare-commit-msg`** â€” Modify the commit message before the editor opens:

| Hook | Repo | What It Does |
|------|------|--------------|
| `commitizen` (cz) | commitizen-tools/commitizen | Interactive commit message prompts |
| Custom template hook | local | Pre-fill commit message from a template |

**`post-checkout` / `post-merge`** â€” Run setup tasks after branch changes:

| Hook | Repo | What It Does |
|------|------|--------------|
| Auto `pip install` | local | Re-install deps after switching branches |
| Auto `npm install` | local | Re-install Node deps |
| DB migration check | local | Warn if unapplied migrations exist |

**`manual`** â€” Opt-in only, run explicitly with `pre-commit run <id> --hook-stage manual`:

| Hook | Repo | What It Does |
|------|------|--------------|
| `typos` | crate-ci/typos | Rust-based spellchecker (stricter than codespell) |
| `markdownlint-cli2` | DavidAnson/markdownlint-cli2 | Markdown linting (Node-based) |
| `hadolint-docker` | hadolint/hadolint | Dockerfile/Containerfile linter |
| `gitleaks` | gitleaks/gitleaks | Secret detection (when not on pre-push) |

#### References

- [Creating new hooks â€” pre-commit docs](https://pre-commit.com/#creating-new-hooks) â€” Official guide to authoring hooks
- [`.pre-commit-hooks.yaml` specification](https://pre-commit.com/#pre-commit-hooks-yaml) â€” Full field reference for hook definitions
- [Supported languages](https://pre-commit.com/#supported-languages) â€” All `language` values and how each is installed/executed
- [Supported hook stages](https://pre-commit.com/#confining-hooks-to-run-at-certain-stages) â€” Details on `stages` field and hook stage configuration
- [Git documentation: githooks](https://git-scm.com/docs/githooks) â€” Underlying Git hooks that pre-commit wraps
- [identify library â€” file type tags](https://github.com/pre-commit/identify) â€” Reference for `types` / `types_or` values

---

## GitHub Actions Workflows

### Anatomy of a Workflow

```yaml
name: Tests                        # Display name

on:                                # Triggers
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:                       # Least-privilege access
  contents: read

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@...  # Pinned SHA
      - uses: actions/setup-python@...
        with:
          python-version: "3.11"
      - run: pip install -e ".[dev]"
      - run: pytest
```

### Common Workflow Patterns

| Workflow | Triggers | Purpose |
|----------|----------|---------|
| `test.yml` | push, PR | Run tests |
| `lint.yml` | push, PR | Ruff, mypy |
| `release.yml` | tag push | Publish to PyPI |
| `security.yml` | schedule, PR | Dependency audits |

### Matrix Testing

Test across multiple Python versions:

```yaml
strategy:
  matrix:
    python-version: ["3.11", "3.12", "3.13"]
    os: [ubuntu-latest, windows-latest]
```

### Caching Dependencies

Speed up workflows by caching pip:

```yaml
- uses: actions/setup-python@...
  with:
    python-version: "3.11"
    cache: "pip"
```

### Useful Actions

| Action | Purpose |
|--------|---------|
| `actions/checkout` | Clone repo |
| `actions/setup-python` | Install Python |
| `actions/cache` | Cache dependencies |
| `codecov/codecov-action` | Upload coverage |
| `pypa/gh-action-pypi-publish` | Publish to PyPI |

### How to Configure Workflow YAML Files

#### Where to learn

The authoritative reference is [GitHub Actions documentation](https://docs.github.com/en/actions).
Key pages:

| Topic | URL |
|-------|-----|
| Workflow syntax | `docs.github.com/en/actions/reference/workflow-syntax-for-github-actions` |
| Events that trigger workflows | `docs.github.com/en/actions/reference/events-that-trigger-workflows` |
| Contexts & expressions | `docs.github.com/en/actions/learn-github-actions/contexts` |
| Permissions | `docs.github.com/en/actions/security-guides/automatic-token-authentication` |
| Encrypted secrets | `docs.github.com/en/actions/security-guides/encrypted-secrets` |
| Variables | `docs.github.com/en/actions/learn-github-actions/variables` |

Each action's own repo README documents its inputs/outputs (e.g., `actions/checkout`, `peter-evans/create-pull-request`).

#### YAML structure at a glance

A workflow file lives in `.github/workflows/` and has four main sections:

```yaml
name: Human-readable name          # Shows in the Actions tab

on:                                 # 1. TRIGGERS â€” when does this run?
  push: ...
  pull_request: ...
  schedule: ...
  workflow_dispatch: ...

permissions:                        # 2. PERMISSIONS â€” least-privilege GITHUB_TOKEN scope
  contents: read

jobs:                               # 3. JOBS â€” what to run (each gets its own runner)
  my-job:
    runs-on: ubuntu-latest
    if: <condition>                  # 4. GUARDS â€” should this job run at all?
    steps:
      - uses: owner/action@sha      # Use a published action
      - run: echo "shell command"    # Run a shell command
```

#### Triggers (`on:`)

| Trigger | When it fires | Notes |
|---------|--------------|-------|
| `push` | Code pushed to matching branches | Can filter by `branches:` and `paths:` |
| `pull_request` | PR opened/synced against matching branches | Uses the **PR head branch's** workflow file |
| `schedule` | Cron expression (UTC) | **Only runs from default branch** (usually `main`) |
| `workflow_dispatch` | Manual "Run workflow" button | Uses the workflow file from the **selected branch** |
| `workflow_run` | After another workflow completes | Useful for chaining workflows |

**Key gotcha:** `schedule:` always uses the workflow file on `main`. If you
change a cron schedule on a branch, it won't take effect until merged.
`workflow_dispatch` does use the selected branch's file, so you can test
workflow changes via the manual trigger before merging.

#### Permissions (least privilege)

Always declare the minimum permissions needed. GitHub's default `GITHUB_TOKEN`
has broad access; narrowing it limits blast radius if a dependency is compromised.

```yaml
permissions:
  contents: read              # Read repo contents (most workflows)
  pull-requests: write        # Create/comment on PRs
  security-events: write      # Upload SARIF to Security tab
  issues: write               # Comment on / close issues
  id-token: write             # OIDC token (OpenSSF Scorecard, cloud auth)
```

**Repo-level setting:** Some permissions also require a repo setting toggle.
Example: "Allow GitHub Actions to create and approve pull requests" must be
enabled at **Settings â†’ Actions â†’ General â†’ Workflow permissions** for any
workflow that creates PRs (like `pre-commit-update.yml`).

#### Guards / Conditionals (`if:`)

Control whether a job runs using expressions:

```yaml
jobs:
  deploy:
    # Only run on the main repo, not forks
    if: github.repository == 'myorg/myrepo'

  auto-merge:
    # Only run for Dependabot PRs
    if: github.actor == 'dependabot[bot]'
```

This project uses a repository guard pattern (see [ADR 011](../adr/011-repository-guard-pattern.md))
to prevent workflows from running on forks that haven't opted in. Template users
can opt in by replacing the slug, setting `vars.ENABLE_WORKFLOWS`, or setting
per-workflow variables.

#### Repository Variables vs Secrets

| Feature | Variables (`vars.*`) | Secrets (`secrets.*`) |
|---------|---------------------|----------------------|
| Visible in logs | Yes | Masked (never printed) |
| Use case | Feature flags, config | API keys, tokens |
| Set at | Settings â†’ Variables | Settings â†’ Secrets |
| Access in YAML | `${{ vars.MY_VAR }}` | `${{ secrets.MY_SECRET }}` |
| Case-sensitive values | Yes (`'true'` â‰  `'True'`) | N/A |

**Gotcha:** Variable comparisons are case-sensitive. `vars.ENABLE_FOO == 'true'`
will not match if the variable is set to `'True'` or `'TRUE'`.

#### Scheduled Workflows (cron)

Cron uses five fields, all in **UTC**:

```
â”Œâ”€â”€â”€ minute (0â€“59)
â”‚ â”Œâ”€â”€â”€ hour (0â€“23)
â”‚ â”‚ â”Œâ”€â”€â”€ day of month (1â€“31)
â”‚ â”‚ â”‚ â”Œâ”€â”€â”€ month (1â€“12)
â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€ day of week (0=Sun â€¦ 6=Sat)
â”‚ â”‚ â”‚ â”‚ â”‚
* * * * *
```

Examples:
- `"0 3 * * *"` â€” daily at 03:00 UTC
- `"0 9 * * 1"` â€” every Monday at 09:00 UTC
- `"0 13 * * 1"` â€” every Monday at 13:00 UTC

Because cron is fixed to UTC, local time shifts with DST (Daylight Saving Time).
For US Eastern: 03:00 UTC = 22:00 EST (Novâ€“Mar) / 23:00 EDT (Marâ€“Nov).

**Important:** Scheduled workflows only run on the default branch. Changing a
cron schedule on a feature branch has no effect until merged.

#### SHA-Pinning Actions

Always pin actions to full commit SHAs, not tags:

```yaml
# BAD â€” tag can be moved to point at malicious code
uses: actions/checkout@v4

# GOOD â€” immutable commit reference
uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
```

The version comment (`# v4.2.2`) is just for humans â€” GitHub resolves the SHA.
If an action maintainer force-pushes or deletes the commit, the SHA becomes
invalid and the workflow will fail at "Set up job" with an error like:
*"An action could not be found at the URI"*.

To find the correct SHA for a release:
1. Go to the action's GitHub releases page
2. Click the tag â†’ click the commit hash
3. Copy the **full 40-character SHA** from the URL

#### Common Mistakes I've Hit

1. **`.dockerignore` excluding build-required files** â€” Hatchling needs
   `README.md` and `LICENSE` during `python -m build`. If your container
   ignore file excludes `*.md` or `LICENSE`, the container build fails
   silently during the wheel step. Fix: add `!README.md` after `*.md` and
   remove `LICENSE` from the exclusion list.

2. **Invalid action SHAs** â€” A pinned SHA that doesn't exist (typo,
   truncated, or force-pushed upstream) causes an immediate failure at
   "Set up job". Always verify the SHA exists on the action's releases page.

3. **Repo setting not enabled** â€” Workflows with `pull-requests: write` or
   that create PRs also need the repo-level "Allow GitHub Actions to create
   and approve pull requests" setting enabled. The workflow permissions in
   YAML are necessary but not sufficient.

4. **Schedule changes on branches** â€” Editing a cron schedule on a feature
   branch does nothing. Scheduled workflows always run from `main`.

5. **Variable case sensitivity** â€” `vars.ENABLE_FOO == 'true'` won't match
   `'True'`. Always use lowercase `'true'` as the convention.

6. **Path-filtered workflows and required checks** â€” If a workflow only runs
   on certain file paths (e.g., `paths: ["src/**"]`), it won't run on PRs
   that don't touch those paths. If that workflow is a required check, the
   PR will hang forever waiting. Solution: exclude path-filtered workflows
   from required checks, or use a CI gate pattern.

---

## Branch Protection

Branch protection prevents direct pushes to important branches and enforces quality gates.

### Setting Up (GitHub)

**Settings â†’ Branches â†’ Add rule**

### Recommended Settings for `main`

| Setting | Purpose |
|---------|---------|
| âœ… Require PR before merging | No direct pushes |
| âœ… Require status checks | CI must pass |
| âœ… Require branches up to date | Must merge main first |
| âœ… Require conversation resolution | All comments addressed |
| â¬œ Require approvals | Set to 1+ for teams |
| â¬œ Restrict who can push | Limit to admins |

### Required Status Checks

Add these as required checks:
- `test` â€” Tests pass
- `lint` â€” Linting passes
- `type-check` â€” Types are correct

### Bypassing (Emergency)

Admins can bypass, but it's logged. Use sparingly!

---

## Security Scanning

### Tools Overview

| Tool | What It Does | When to Run |
|------|--------------|-------------|
| **pip-audit** | Checks deps for CVEs | CI, pre-release |
| **Bandit** | Finds security bugs in code | CI, pre-commit |
| **Safety** | Dependency vulnerabilities | CI |
| **Trivy** | Container scanning | CI (Docker builds) |
| **Dependabot** | Auto-creates upgrade PRs | Scheduled |

### What Is SARIF?

**SARIF** (Static Analysis Results Interchange Format) is a standardized JSON format for the output of static analysis tools. It's an [OASIS standard](https://docs.oasis-open.org/sarif/sarif/v2.1.0/sarif-v2.1.0.html) designed so that different tools (linters, security scanners, type checkers) can all produce results in the same shape.

**Why it matters:**
- **GitHub Security tab** â€” When a CI workflow uploads a `.sarif` file via `github/codeql-action/upload-sarif`, the findings appear as **Code scanning alerts** in the repository's Security tab. This gives a unified view of vulnerabilities across tools.
- **Tool-agnostic** â€” Whether results come from Trivy, Grype, Bandit, CodeQL, or Scorecard, SARIF normalises them into one format.
- **IDE integration** â€” VS Code extensions (e.g., SARIF Viewer) can display SARIF results inline, showing issues right where the code is.

**Structure:** A SARIF file contains `runs`, each with a `tool` descriptor and an array of `results`. Each result has a `ruleId`, `message`, `level` (error/warning/note), and `locations` pointing to specific files and line numbers.

**In this project:** The `scorecard.yml`, `container-scan.yml`, and `nightly-security.yml` workflows all produce SARIF output and upload it to GitHub's Security tab.

### pip-audit in CI

```yaml
- name: Security audit
  run: |
    pip install pip-audit
    pip-audit
```

### Bandit in CI

```yaml
- name: Security scan
  run: |
    pip install bandit
    bandit -r src/ -ll
```

### Dependabot Configuration (`.github/dependabot.yml`)

```yaml
version: 2
updates:
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "weekly"
    groups:
      dev-dependencies:
        patterns:
          - "pytest*"
          - "ruff"
          - "mypy"

  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"
```

---

## Automation Strategy

### The Three Lines of Defense

1. **Pre-commit hooks** â€” Catch issues locally, instant feedback
2. **CI workflows** â€” Catch anything that slips through, authoritative
3. **Branch protection** â€” Enforce that CI passes before merge

### What to Run Where

| Check | Pre-commit | CI | Why |
|-------|------------|-----|-----|
| Formatting | âœ… | âœ… | Fast, catches everything |
| Linting | âœ… | âœ… | Fast, catches everything |
| Type checking | âš ï¸ Optional | âœ… | Can be slow locally |
| Tests | âŒ | âœ… | Too slow for commit hook |
| Security scan | âŒ | âœ… | Needs network, slow |
| Coverage | âŒ | âœ… | Needs full test run |

### Progressive Adoption

1. **Start with CI** â€” Get workflows running first
2. **Add branch protection** â€” Enforce CI passes
3. **Add pre-commit** â€” Speed up feedback loop
4. **Tune thresholds** â€” Gradually increase strictness

---

## GitHub README Priority Order

GitHub has a hidden priority order for which README displays on the repo landing page:

1. **`.github/README.md`** â€” Highest priority (surprising!)
2. **`README.md`** â€” Root (what you'd expect)
3. **`docs/README.md`** â€” Lowest priority

**The gotcha:** If you put a `README.md` in `.github/` to document your workflows and templates, it **silently replaces** your root `README.md` on the repository page. Visitors see your internal `.github/` docs instead of your project README â€” with no warning.

This is unique to `.github/`. Every other directory's `README.md` only renders when browsing *that* directory. But `.github/README.md` is treated as a profile-level README, similar to how `<username>/<username>/README.md` shows on your GitHub profile.

**Fix:** Don't put a `README.md` in `.github/`. Document that directory's contents elsewhere (e.g., `docs/repo-layout.md`).

See: [ADR 015](../adr/015-no-github-directory-readme.md)

---

## README Badges

Badges are small status images in your README that show project health at a glance.
They're generated dynamically by external services and rendered as inline images.

### Common Badge Types

| Badge | What it shows | Service |
|-------|---------------|---------|
| CI Status | Whether tests/checks pass | GitHub Actions |
| Coverage | Test coverage percentage | Codecov, Coveralls |
| License | Project license | Shields.io |
| Python Version | Supported versions | Shields.io |
| Code Style | Formatter/linter used | Shields.io |
| Downloads | PyPI download count | PyPI, pepy.tech |
| Version | Latest release | GitHub, PyPI |

### How Badges Work

1. You add a Markdown image link: `[![alt](image-url)](click-url)`
2. The image URL points to a service that returns a dynamically-generated SVG
3. GitHub renders the SVG inline in your README
4. The click URL takes users to the full details

### Badge Anatomy

```markdown
[![CI](https://github.com/OWNER/REPO/actions/workflows/ci-gate.yml/badge.svg)](https://github.com/OWNER/REPO/actions/workflows/ci-gate.yml)
 â”‚    â”‚                                                                        â”‚
 â”‚    â””â”€â”€ Image URL (returns SVG)                                              â””â”€â”€ Click URL (where badge links to)
 â””â”€â”€ Alt text (shown if image fails)
```

### GitHub Actions Badge

```markdown
[![CI](https://github.com/OWNER/REPO/actions/workflows/WORKFLOW.yml/badge.svg)](https://github.com/OWNER/REPO/actions/workflows/WORKFLOW.yml)
```

Shows: passing/failing based on most recent workflow run on default branch.

**Gotcha:** Won't show anything until the workflow has run at least once on main.

### Codecov Badge

```markdown
[![Coverage](https://codecov.io/gh/OWNER/REPO/branch/main/graph/badge.svg)](https://codecov.io/gh/OWNER/REPO)
```

Shows: Coverage percentage from latest upload.

**Gotcha:** Requires Codecov account and at least one coverage upload.

### Shields.io (Static/Dynamic Badges)

Shields.io generates badges for almost anything:

```markdown
<!-- Static badge (hardcoded values) -->
[![Python](https://img.shields.io/badge/python-3.11%2B-blue)](https://python.org)

<!-- Dynamic badge (fetches from GitHub API) -->
[![License](https://img.shields.io/github/license/OWNER/REPO)](LICENSE)
[![Stars](https://img.shields.io/github/stars/OWNER/REPO)](https://github.com/OWNER/REPO)
[![Issues](https://img.shields.io/github/issues/OWNER/REPO)](https://github.com/OWNER/REPO/issues)
```

### Badge Best Practices

1. **Keep it minimal** â€” 3-6 badges max. Too many creates visual noise.
2. **Put important ones first** â€” CI status, coverage, then others.
3. **Use consistent styling** â€” Shields.io has style options (`?style=flat-square`).
4. **Test before committing** â€” Paste URLs in browser to verify they work.
5. **Check on both light/dark themes** â€” Some badges look bad on dark mode.

### Troubleshooting

| Problem | Cause | Fix |
|---------|-------|-----|
| Badge shows "no status" | Workflow never ran | Push to main to trigger workflow |
| Badge shows old data | Caching | Add `?cache=no` or wait ~5 min |
| Coverage badge broken | No Codecov setup | Sign up at codecov.io, add token |
| Badge looks pixelated | Using PNG | Use SVG URL instead |
| Badge 404 | Wrong URL | Check owner/repo spelling and case |

### Codecov Setup

Codecov is **free for public repos** and has a free tier for private repos.

**Sign up:**
1. Go to [codecov.io](https://codecov.io)
2. Click "Sign up" â†’ "Sign up with GitHub"
3. Authorize Codecov to access your repos
4. Add your repo from the dashboard

**For public repos:** Works automatically after first coverage upload.

**For private repos:** Copy the `CODECOV_TOKEN` from Codecov dashboard and add it
as a repository secret in GitHub (Settings â†’ Secrets â†’ Actions â†’ New secret).

---

## Things I Keep Forgetting

1. **Import name â‰  package name** â€” `simple-python-boilerplate` (hyphen) installs, but you `import simple_python_boilerplate` (underscore)

2. **`__init__.py` is still needed** â€” Even in Python 3, include it for tooling compatibility

3. **Editable install is required** â€” With `src/` layout, you must install to import

4. **pytest needs the package installed** â€” Or it won't find your modules

5. **Shebang + executable bit on Windows** â€” When creating a new script with a
   shebang (`#!/usr/bin/env python3`), Windows doesn't have `chmod +x`. Git
   tracks the executable permission though, so you need
   `git add --chmod=+x scripts/your_script.py` to set it. Once committed, anyone
   who clones or uses the template gets the bit automatically â€” it's a one-time
   thing per new script file on the authoring side only.

---

## Research: Other Template Repos

Notes and conventions gathered from popular Python boilerplate/template repositories on GitHub.

---

### Hypermodern Python (cjolowicz)

**Repo:** [cjolowicz/hypermodern-python](https://github.com/cjolowicz/hypermodern-python)

A reference for cutting-edge Python tooling. Accompanied by a detailed blog series.

| Convention | Details |
|------------|---------|
| **Build tool** | Poetry (now Hatch is also popular) |
| **Task runner** | Nox (multi-Python testing) |
| **Type checker** | Mypy with strict mode |
| **Docs** | Sphinx + Read the Docs |
| **Pre-commit** | Extensive hooks |
| **CLI** | Click |

**Key Takeaways:**
- Uses Nox for consistent test environments across Python versions
- Separates "sessions" (lint, tests, docs, safety) in `noxfile.py`
- Coverage enforced with pytest-cov
- Sphinx autodoc for API docs from docstrings
- GitHub Actions with matrix for Python 3.9â€“3.12

---

### Cookiecutter PyPackage (audreyfeldroy)

**Repo:** [audreyfeldroy/cookiecutter-pypackage](https://github.com/audreyfeldroy/cookiecutter-pypackage)

One of the most popular Python package templates. Uses Cookiecutter for project generation.

| Convention | Details |
|------------|---------|
| **Layout** | `src/` layout (optional, flat by default) |
| **Testing** | pytest + tox |
| **Docs** | Sphinx |
| **CI** | Travis CI (older), GitHub Actions (forks) |
| **Versioning** | bumpversion |

**Key Takeaways:**
- Generates CONTRIBUTING.rst, HISTORY.rst, AUTHORS.rst
- Includes Makefile with common targets (`make test`, `make docs`)
- Supports multiple open source licenses via prompts
- tox.ini for multi-version testing
- Bump2version for version management

---

### Python Project Template (rochacbruno)

**Repo:** [rochacbruno/python-project-template](https://github.com/rochacbruno/python-project-template)

Modern template with Copier (alternative to Cookiecutter).

| Convention | Details |
|------------|---------|
| **Build tool** | Poetry or setuptools |
| **Linting** | Ruff (replaced flake8, isort, black) |
| **Type checker** | Mypy |
| **Task runner** | Make |
| **Docs** | MkDocs (Material theme) |

**Key Takeaways:**
- Uses Copier for template updates (can pull in template changes later)
- Containerfile for OCI/Docker builds
- GitHub Actions with reusable workflows
- Conventional commits enforced
- MkDocs Material for modern-looking docs

---

### FastAPI Project Structure (tiangolo)

**Repo:** [tiangolo/full-stack-fastapi-template](https://github.com/tiangolo/full-stack-fastapi-template)

While not a general template, FastAPI projects set conventions for modern Python.

| Convention | Details |
|------------|---------|
| **Layout** | `app/` package (not `src/`) |
| **Async** | Native async/await |
| **Config** | Pydantic Settings |
| **Database** | SQLAlchemy + Alembic |
| **Testing** | pytest + httpx |

**Key Takeaways:**
- Pydantic for settings/config management with `.env` files
- Alembic for database migrations (instead of raw SQL)
- Docker Compose for local development
- Separation: `app/core/`, `app/api/`, `app/models/`, `app/crud/`
- Pre-commit with Ruff

---

### Scikit-learn Contrib Template

**Repo:** [scikit-learn-contrib/project-template](https://github.com/scikit-learn-contrib/project-template)

Template for scikit-learn compatible packages.

| Convention | Details |
|------------|---------|
| **Layout** | Flat (package at root) |
| **Testing** | pytest-cov |
| **Docs** | Sphinx + sphinx-gallery |
| **CI** | GitHub Actions + CircleCI |

**Key Takeaways:**
- Strict scikit-learn API compatibility (estimator checks)
- Example gallery generated from scripts
- Extensive docstring format (NumPy style)
- Conda + pip dual support

---

### PyScaffold

**Repo:** [pyscaffold/pyscaffold](https://github.com/pyscaffold/pyscaffold)

CLI tool that generates Python projects. Very opinionated.

| Convention | Details |
|------------|---------|
| **Layout** | `src/` layout (enforced) |
| **Config** | `pyproject.toml` + `setup.cfg` hybrid |
| **Versioning** | setuptools-scm (git tags) |
| **Docs** | Sphinx |
| **Extensions** | Plugin system |

**Key Takeaways:**
- Version derived from git tags (no manual version bumping)
- setuptools-scm for automatic versioning
- Extensions for Django, pre-commit, CI templates
- Creates `CHANGELOG.rst` (reStructuredText)
- Authors file auto-generated from git log

---

### Packaging Conventions Comparison

| Aspect | This Template | Hypermodern | Cookiecutter | PyScaffold |
|--------|---------------|-------------|--------------|------------|
| **Layout** | `src/` | `src/` | flat (default) | `src/` |
| **Config** | pyproject.toml only | pyproject.toml | setup.py/cfg | hybrid |
| **Linting** | Ruff | flake8 + plugins | flake8 | flake8 |
| **Formatting** | Ruff | Black | Black | Black |
| **Types** | Mypy | Mypy strict | optional | optional |
| **Task runner** | Make/scripts | Nox | Make + tox | tox |
| **Docs** | Markdown | Sphinx | Sphinx | Sphinx |
| **Versioning** | manual | bump2version | bumpversion | setuptools-scm |

---

### Common Patterns Observed

**Project Structure:**
- `src/` layout gaining popularity (isolation benefits)
- `tests/` at root level (not inside `src/`)
- `docs/` for documentation source files
- Flat configs in root (pyproject.toml, tox.ini, etc.)

**Configuration:**
- pyproject.toml as single source (PEP 518/621)
- Tool configs in `[tool.X]` sections
- `.env` + `.env.example` for secrets

**CI/CD:**
- GitHub Actions dominant (Travis CI declining)
- Matrix testing (Python 3.10, 3.11, 3.12, etc.)
- Separate workflows per concern
- Dependabot or Renovate for dependency updates

**Documentation:**
- Sphinx still dominant for libraries
- MkDocs + Material gaining traction
- README.md as landing page
- CHANGELOG.md with Keep a Changelog format

**Testing:**
- pytest universal
- pytest-cov for coverage
- tox or nox for multi-version
- conftest.py for shared fixtures

**Developer Experience:**
- Pre-commit hooks standard
- Makefile or justfile for common tasks
- Editorconfig for cross-editor consistency
- .vscode/ or .idea/ for IDE settings

---

### Ideas Worth Considering

From researching these templates, potential additions:

| Idea | Benefit | Complexity |
|------|---------|------------|
| **setuptools-scm** | No manual version bumping | Low |
| **Nox** | Better than tox, Python-based | Medium |
| **MkDocs** | Simpler than Sphinx, Markdown-native | Low |
| **Copier** | Template updates after adoption | Medium |
| **justfile** | Modern Makefile alternative | Low |
| **CITATION.cff** | Academic citation support | Low |
| **.editorconfig** | Cross-editor consistency | Low |

---

### Other Template Repos (To Review)

Additional template and boilerplate repositories worth studying:

---

### PyPA Sample Project

**Repo:** [pypa/sampleproject](https://github.com/pypa/sampleproject)

The official sample project from the Python Packaging Authority (PyPA). Exists as a companion to the [PyPUG Tutorial on Packaging and Distributing Projects](https://packaging.python.org/tutorials/packaging-projects/). Intentionally minimal â€” focuses purely on packaging, not project development practices.

| Convention | Details |
|------------|---------|
| **Layout** | `src/` layout (`src/sample/`) |
| **Config** | `pyproject.toml` only |
| **Testing** | Nox |
| **CI** | GitHub Actions |
| **Docs** | None (README only) |
| **Versioning** | Manual in `pyproject.toml` |

**Key Takeaways:**
- The most authoritative reference for pyproject.toml packaging metadata
- Deliberately does not cover linting, formatting, type checking, or CI beyond testing
- `src/` layout used as the recommended default
- 5.3k stars â€” extremely well-known as the canonical packaging example
- Good reference for pyproject.toml fields (classifiers, URLs, optional-dependencies, entry-points)
- No template engine â€” just a plain project to clone and adapt

---

### Kotlin Android Template (cortinico)

**Repo:** [cortinico/kotlin-android-template](https://github.com/cortinico/kotlin-android-template)

A 100% Kotlin template for Android projects with static analysis and CI baked in. Useful as a cross-language comparison for how non-Python ecosystems approach project templates.

| Convention | Details |
|------------|---------|
| **Language** | Kotlin (100%) |
| **Build tool** | Gradle (Kotlin DSL) |
| **Static analysis** | Detekt + ktlint |
| **CI** | GitHub Actions (pre-merge, publish-snapshot, publish-release) |
| **Dependency management** | Gradle Version Catalog (`libs.versions.toml`) + Renovate |
| **Publishing** | Maven Central via Nexus |

**Key Takeaways:**
- Multi-module structure: `app/`, `library-android/`, `library-kotlin/`, `library-compose/`
- Shared build logic lives in `buildSrc/` as precompiled script plugins
- Renovate (not Dependabot) for automated dependency updates with auto-merge
- Static analysis runs as part of CI, not just pre-commit
- Jetpack Compose module included as a ready-to-use example
- No template engine â€” uses GitHub "Use this template" with manual find-and-replace
- `.idea/` directory committed for consistent IDE settings (Android Studio / IntelliJ)
- 1.9k stars; maintained by a Meta/React Native engineer

---

### Electron Boilerplate (sindresorhus)

**Repo:** [sindresorhus/electron-boilerplate](https://github.com/sindresorhus/electron-boilerplate) (**Archived** May 2024)

A minimal, opinionated Electron starter from sindresorhus. Now archived but still a good reference for how a "less is more" boilerplate can work.

| Convention | Details |
|------------|---------|
| **Language** | JavaScript (84%), CSS, HTML |
| **Build tool** | electron-builder |
| **CI** | GitHub Actions (cross-platform builds) |
| **Config** | electron-store |
| **Error handling** | electron-unhandled |
| **Editor** | `.editorconfig` |

**Key Takeaways:**
- Extremely minimal â€” only the files you actually need (no over-engineering)
- electron-builder configured for cross-platform builds (macOS, Linux, Windows)
- Silent auto-updates built in
- System-native app menu out of the box
- Context menu via `electron-context-menu`
- README acts as a template itself â€” "remove everything above here" pattern
- Example of a successful boilerplate that is a working app, not a meta-template
- 1.6k stars; archived because the author moved on from Electron

---

### Josee9988's Project Template

**Repo:** [Josee9988/project-template](https://github.com/Josee9988/project-template)

A language-agnostic GitHub template focused on community health files, issue templates, labels, and repository automation. Not about code structure â€” about the GitHub repo wrapper around a project.

| Convention | Details |
|------------|---------|
| **Language** | Language-agnostic (Shell for setup script) |
| **Setup** | `SETUP_TEMPLATE.sh` script auto-detects and replaces placeholders |
| **Issue templates** | 8 templates (bug, failing test, docs, feature, enhancement, security, question, blank) |
| **Labels** | 20+ labels auto-created via `settings.yml` bot |
| **Community files** | CODE_OF_CONDUCT, CONTRIBUTING, SECURITY, SUPPORT, CODEOWNERS |
| **Bots** | issue-label-bot, probot-settings, welcome-bot, todo-bot |

**Key Takeaways:**
- Strongest emphasis on community health files of any template reviewed
- Shell script for initial personalisation (replaces placeholders in all files)
- Uses GitHub Probot ecosystem heavily for automation
- CHANGELOG follows Keep a Changelog format
- Pull request template auto-closes linked issues via keywords
- No CI workflows for code â€” purely a "repo wrapper" template
- 934 stars; last updated 4 years ago (not actively maintained)

---

### inovintell Python Template

**Repo:** [inovintell/py-template](https://github.com/inovintell/py-template)

A Python template that uses **Cookiecutter** for project generation. Focused on CI/CD automation with semantic release and comprehensive GitHub Actions pipelines.

| Convention | Details |
|------------|---------|
| **Layout** | `src/{{cookiecutter.repository}}/` (Cookiecutter-templated) |
| **Build tool** | Poetry |
| **Linting** | Ruff |
| **Pre-commit** | Yes (with commitlint for conventional commits) |
| **CI** | GitHub Actions (extensive pipeline) |
| **Docs** | MkDocs |
| **Dependency updates** | Renovate |
| **Versioning** | Semantic release (automated) |

**Key Takeaways:**
- Uses Cookiecutter â€” source files have `{{cookiecutter.repository}}` placeholders throughout
- This means the template repo itself is not directly runnable or testable
- Heavy Renovate usage â€” bot commits dominate the commit history
- Conventional commits enforced via commitlint
- `.editorconfig` and `.yamllint.yml` included for cross-editor and YAML consistency
- Poetry for dependency management (not pip/setuptools)
- Example of the trade-off ADR-014 discusses: powerful automation but harder to read/contribute to
- 92 stars; actively maintained (Renovate keeps dependencies current)

---

### Awesome Repo Template (MarketingPipeline)

**Repo:** [MarketingPipeline/Awesome-Repo-Template](https://github.com/MarketingPipeline/Awesome-Repo-Template)

A feature-rich, language-agnostic GitHub template with heavy automation via GitHub Actions workflows that run at template setup time.

| Convention | Details |
|------------|---------|
| **Language** | Language-agnostic (HTML landing page) |
| **Setup** | GitHub Actions workflow auto-replaces links, emails, and metadata |
| **Community files** | CODE_OF_CONDUCT, CONTRIBUTING, SECURITY, CODEOWNERS, CHANGE_LOG, TO_DO |
| **Issue templates** | Bug report and feature request (YAML-based forms) |
| **Automation** | Image compression, TOC generation, stargazer metrics SVG, SEO index.html |
| **Bots** | issue-label-bot, probot-settings, welcome-bot |

**Key Takeaways:**
- Unique approach: uses a GitHub Actions workflow to auto-configure the repo after creation
- Generates a markdown-styled `index.html` with SEO metadata
- Auto-generates a table of contents in the README
- Stargazer metrics SVG generated automatically
- Image optimisation workflow compresses all repo images
- More "GitHub infrastructure" than "code template" â€” no language-specific tooling
- Similar to Josee9988's template but with more workflow-based automation
- 201 stars; last updated 4 years ago

---

## Source Code File Workflow

A clean separation of concerns for the `src/` package structure.

### The Pattern

```
main.py   â†’ starts the program (entry points, thin wrappers)
cli.py    â†’ defines CLI contract (argument parsing, commands)
engine.py â†’ defines behavior (core logic, interface-agnostic)
api.py    â†’ defines callable interface (HTTP/REST, optional)
```

### File Responsibilities

| File | Purpose | Contains |
|------|---------|----------|
| `main.py` | Entry points | Thin wrappers that call cli/engine |
| `cli.py` | CLI contract | Argument parser, command definitions |
| `engine.py` | Behavior | Pure logic, no I/O, easily testable |
| `api.py` | API interface | HTTP routes, request/response handling |

### Data Flow

```
User runs command
       â†“
main.py (entry point)
       â†“
cli.py (parse args, dispatch)
       â†“
engine.py (do the work)
       â†“
Return result to cli.py
       â†“
Format output (cli.py or main.py)
       â†“
User sees result
```

### Why This Pattern?

1. **Testability** â€” `engine.py` has no CLI/HTTP dependencies, easy to unit test
2. **Flexibility** â€” Same engine can power CLI, API, GUI, etc.
3. **Clarity** â€” Each file has one job
4. **Maintainability** â€” Changes to CLI don't affect core logic

### Example

```python
# engine.py â€” pure logic
def process_data(data: str) -> str:
    return f"Processed: {data}"

# cli.py â€” CLI contract
def run(args):
    from engine import process_data
    result = process_data(args.input)
    print(result)
    return 0

# main.py â€” entry point
def main():
    from cli import parse_args, run
    sys.exit(run(parse_args()))
```

### Anti-patterns to Avoid

- âŒ Business logic in `main.py`
- âŒ Argument parsing in `engine.py`
- âŒ HTTP-specific code in `engine.py`
- âŒ `print()` statements in `engine.py` (return data instead)

---

| File             | Primary role                  | What it contains                                                            | What it must **not** contain                                     | Who calls it                        | When to use it                                     | Common mistakes                                          |
|------------------|-------------------------------|-----------------------------------------------------------------------------|------------------------------------------------------------------|-------------------------------------|----------------------------------------------------|----------------------------------------------------------|
| **`engine.py`**  | Source of truth (core logic)  | Pure functions/classes that implement real behavior                         | CLI parsing, printing, shell commands, repo-specific assumptions | `api.py`, tests, other Python code  | Always when behavior is non-trivial or reusable    | Mixing I/O or argument parsing into core logic           |
| **`api.py`**     | Stable internal interface     | Thin wrappers that expose intentional operations (e.g. `run_lint`, `build`) | Implementation details, argument parsing                         | `cli.py`, `main.py`, other tools    | When you want a clean boundary and refactor safety | Making it a duplicate of `engine.py` with no added value |
| **`cli.py`**     | Command-line interface        | Argument parsing, subcommands, help text                                    | Business logic, complex workflows                                | End users, developers, Just, CI     | When providing an installable CLI                  | Putting real logic directly in CLI handlers              |
| **`main.py`**    | Entry point / bootstrap       | Calls into `api.py` or `engine.py` to start execution                       | Logic, configuration rules                                       | Python runtime (`python main.py`)   | Optional; useful for quick execution or demos      | Letting it grow into the main implementation file        |

### Key Rule

> Logic flows downward; control flows upward.

- Logic lives in `engine.py`
- Interfaces adapt it (`api.py`, `cli.py`)
- Entrypoints trigger it (`main.py`)

### Decision Rules (read top â†’ bottom)

- If someone outside this repo needs to run it â†’ installable CLI
- If only contributors need it â†’ task runner (Just)
- If it expresses real behavior â†’ core logic
- If it just wires things together â†’ orchestration

### Canonical Decision Table

| Question                                                        | Yes â†’ Do this                                       | No â†’ Do this |
|-----------------------------------------------------------------|-----------------------------------------------------|--------------|
| Does this define real behavior (rules, algorithms, decisions)?  | Put it in **core logic** (`engine.py` / `core/`)    | Continue     |
| Should this behavior be callable by other code or tools?        | Expose via **installable CLI** (and/or API)         | Continue     |
| Is this meant to be run outside this repo?                      | **Installable CLI command**                         | Continue     |
| Is this only for contributors working on this repo?             | **Just task**                                       | Continue     |
| Is this repo-specific glue (order of steps, flags, paths)?      | **Just task or script**                             | Continue     |
| Is this a one-off or disposable automation?                     | **Script**                                          | Re-evaluate  |

### What Each Bucket Is Responsible For

| Tool / Layer       | Purpose                  | Source of truth? | Versioned? | Audience     |
|--------------------|--------------------------|------------------|------------|-------------|
| Core logic         | Implements behavior      | âœ… Yes            | With code  | Everyone     |
| Installable CLI    | Defines public commands  | âœ… Yes            | Yes        | Users / devs |
| Just (task runner) | Orchestrates commands    | âŒ No             | With repo  | Contributors |
| Scripts            | One-off helpers          | âŒ No             | Optional   | Maintainers  |
| CI workflows       | Automation               | âŒ No             | With repo  | CI only      |

### Concrete Examples (grounding the rules)

| Action                       | Correct place                    | Why                          |
|------------------------------|----------------------------------|------------------------------|
| Lint Python files            | Installable CLI (`mytool lint`)  | Reusable, meaningful behavior |
| Run lint + format + tests    | Just (`just check`)              | Repo workflow                |
| Build and publish release    | CLI (`mytool release`)           | Stable, versioned behavior   |
| Clean `.pytest_cache`        | Just or script                   | Repo-specific cleanup        |
| Bootstrap venv               | Just                             | Developer convenience        |
| Parse config file            | Core logic                       | Behavior, not orchestration  |
| Call multiple tools in order | Just                             | Pure glue                    |

### Anti-patterns (what not to do)

| Smell                                  | Why it's wrong                   |
|----------------------------------------|----------------------------------|
| Logic lives in `justfile`              | Not testable or reusable         |
| CI runs `just something`               | CI now depends on dev tooling    |
| CLI calls shell pipelines              | Logic trapped in strings         |
| Scripts are the only interface         | No stable API                    |
| Just command documented as "the way"   | Just became the API              |

### One-sentence Rule (worth memorizing)

> Installable CLIs define behavior.
> Just coordinates behavior.
> Scripts are temporary.

### Why This Matters for Your Template

You are not just writing codeâ€”you are teaching architecture.

**If you teach:**

- "put logic in core"
- "keep runners dumb"

**Then users:**

- Can refactor safely
- Can add new interfaces later
- Avoid brittle repos

---

## Common Python Cache & Artifact Directories

| Path               | Created by          | Purpose                                            | Safe to delete? | Commit to git? |
|--------------------|---------------------|----------------------------------------------------|-----------------|----------------|
| `__pycache__/`     | Python interpreter  | Stores compiled `.pyc` bytecode for faster imports | âœ… Yes           | âŒ Never        |
| `.pytest_cache/`   | pytest              | Remembers test state (last failed, node IDs)       | âœ… Yes           | âŒ Never        |
| `.mypy_cache/`     | mypy                | Type-checking cache                                | âœ… Yes           | âŒ Never        |
| `.ruff_cache/`     | ruff                | Linting cache                                      | âœ… Yes           | âŒ Never        |
| `.coverage`        | coverage.py         | Coverage data file                                 | âœ… Yes           | âŒ Never        |
| `htmlcov/`         | coverage.py         | HTML coverage report                               | âœ… Yes           | âŒ Never        |
| `.tox/`            | tox                 | Virtualenvs + test environments                    | âœ… Yes           | âŒ Never        |
| `.nox/`            | nox                 | Virtualenvs + sessions                             | âœ… Yes           | âŒ Never        |
| `.venv/`           | venv / uv / poetry  | Local virtual environment                          | âœ… Yes           | âŒ Never        |
| `dist/`            | build tools         | Built distributions (wheel/sdist)                  | âœ… Yes           | âŒ Never        |
| `build/`           | build tools         | Temporary build artifacts                          | âœ… Yes           | âŒ Never        |

### Why Python Creates So Many Caches

Python tooling is modular:

- Each tool optimizes independently
- Each tool owns its own cache
- No central "build system" cleans everything automatically

This is normal and healthy.

### Do Other Programming Languages Have the Same Thing?

Yes â€” absolutely. Every serious ecosystem does.

**Comparison across ecosystems:**

| Language    | Examples of cache / artifact dirs                              |
|-------------|----------------------------------------------------------------|
| Python      | `__pycache__/`, `.pytest_cache/`, `.mypy_cache/`, `.venv/`     |
| JavaScript  | `node_modules/`, `.next/`, `.turbo/`, `.parcel-cache/`         |
| Rust        | `target/`                                                      |
| Java        | `target/`, `.gradle/`                                          |
| Go          | `pkg/`, `bin/`, module cache                                   |
| C/C++       | `build/`, `*.o`, `*.a`, `*.out`                                |
| .NET        | `bin/`, `obj/`                                                 |

The names differ; the idea is identical.

### Why These Should Never Be Committed

Cache directories are:

- Machine-specific
- Non-deterministic
- Frequently invalidated
- Huge source of merge conflicts

A repo that commits caches is a broken repo.

### Where Cleanup Belongs (Architecture Tie-In)

Cleaning caches:

- Is not real behavior
- Is not business logic
- Is repo hygiene

**Correct places:**

- `just clean`
- `scripts/clean.py`
- CI steps

**Incorrect places:**

- Core logic
- Installable CLI
- Application code

### Example `.gitignore` Entries

```gitignore
# Python
__pycache__/
*.py[cod]

# Virtual environments
.venv/
env/

# Test / lint / type-check caches
.pytest_cache/
.mypy_cache/
.ruff_cache/
.coverage
htmlcov/

# Build artifacts
build/
dist/
*.egg-info/
```

### One Rule to Remember

> If deleting it breaks nothing permanently, it's not source code.

---

## What is `*.egg-info`?

`*.egg-info/` is Python packaging metadata generated by setuptools (often via pip) when a project is installed, especially in editable (`pip install -e .`) mode.

It is not source code. It is installation metadata.

### What Lives Inside `*.egg-info/`

Typical contents:

```
my_package.egg-info/
â”œâ”€â”€ PKG-INFO        # Name, version, license, metadata
â”œâ”€â”€ SOURCES.txt     # Files included in the package
â”œâ”€â”€ requires.txt    # Dependencies
â”œâ”€â”€ entry_points.txt# Console scripts / CLI entry points
â”œâ”€â”€ top_level.txt   # Top-level import names
â””â”€â”€ dependency_links.txt
```

This data answers questions like:

- "What version is installed?"
- "What console scripts should exist?"
- "What are the dependencies?"

### Why It Appears in Your Repo

Common causes:

- You ran `pip install -e .`
- A tool installed your package locally
- A test or dev workflow installed the project

Editable installs must write metadata somewhere, and `*.egg-info` is how setuptools does it.

### Should `*.egg-info` Be Committed?

**No. Never.**

Reasons:

- Machine-specific paths
- Installation-state dependent
- Regenerated at will
- Causes merge noise and confusion

Add to `.gitignore`:

```gitignore
*.egg-info/
```

### Related Packaging Artifacts (same category)

These are not caches, but build/install artifacts.

| Artifact              | Created by  | Purpose                     | Commit? |
|-----------------------|-------------|-----------------------------|:-------:|
| `*.egg-info/`         | setuptools  | Installed package metadata  | âŒ       |
| `*.dist-info/`        | pip         | Wheel installation metadata | âŒ       |
| `dist/`               | build tools | Built wheels / sdists       | âŒ       |
| `build/`              | build tools | Temporary build output      | âŒ       |
| `pip-wheel-metadata/` | pip         | Intermediate wheel metadata | âŒ       |

**Rule:** If it only exists after install or build, it does not belong in git.

### `egg-info` vs `dist-info` (important distinction)

**`*.egg-info`**

- Legacy / setuptools-era format
- Common in editable installs

**`*.dist-info`**

- Modern standard (PEP 376)
- Created when installing wheels

Both serve the same role: describe whatâ€™s installed, not what you wrote.

### Beyond Python: Other Non-Cache Artifacts You May See

These exist in many ecosystems and are not caches, but tooling state.

| Ecosystem   | Examples                              |
|-------------|---------------------------------------|
| Python      | `*.egg-info/`, `*.dist-info/`         |
| JavaScript  | `package-lock.json`, `pnpm-lock.yaml` |
| Rust        | `Cargo.lock`                          |
| Java        | `pom.xml`, `.classpath`               |
| .NET        | `.csproj`, `.deps.json`               |

**Key difference:**

- Lockfiles â†’ usually committed
- Install/build metadata â†’ never committed

### Mental Model (use this)

> Caches speed things up.
> Metadata describes installed artifacts.
> Neither is source code.

If deleting it only requires reinstalling or rebuilding â†’ it does not belong in git.

### Where This Fits in Your Architecture Rules

`*.egg-info` is not:

- Logic
- CLI
- Just
- Scripts

It is a tool byproduct, managed by the packaging system.

### Bottom-Line Rules

- Source code â†’ commit
- Configuration â†’ commit
- Lockfiles â†’ usually commit
- Caches â†’ never commit
- Build artifacts â†’ never commit
- Install metadata (`*.egg-info`, `*.dist-info`) â†’ never commit

### Quick Reference: When to Use What

| Scenario              | Installable CLI | Just | Script |
|-----------------------|-----------------|------|--------|
| Reusable logic        | âœ…               | âŒ    | âŒ      |
| Distributed tool      | âœ…               | âŒ    | âŒ      |
| Repo glue             | âŒ               | âœ…    | âš ï¸     |
| One-off automation    | âŒ               | âš ï¸   | âœ…      |
| User-facing command   | âœ…               | âŒ    | âŒ      |
| Developer convenience | âš ï¸              | âœ…    | âš ï¸     |

---

## What is `pyproject.toml`?

`pyproject.toml` is a single configuration file (written in [TOML](https://toml.io/)) that defines everything about a Python project: metadata, dependencies, build instructions, and tool settings.

Before `pyproject.toml`, Python projects needed multiple config files (`setup.py`, `setup.cfg`, `tox.ini`, `.flake8`, `mypy.ini`, etc.). Now most of that lives in one place.

### The Standards Behind It

| PEP | What It Defines | Year |
|-----|----------------|------|
| [PEP 518](https://peps.python.org/pep-0518/) | `[build-system]` table â€” how to build the project | 2016 |
| [PEP 621](https://peps.python.org/pep-0621/) | `[project]` table â€” project metadata (name, version, deps, etc.) | 2020 |
| [PEP 517](https://peps.python.org/pep-0517/) | Build backend interface (how pip talks to build tools) | 2017 |
| [PEP 660](https://peps.python.org/pep-0660/) | Editable installs via build backends | 2021 |

These PEPs made `pyproject.toml` the **standard** way to configure Python projects. Any PEP 621-compliant tool (pip, Hatch, setuptools, Flit, PDM, Dependabot, etc.) can read the `[project]` table.

### Structure Overview

A `pyproject.toml` has three major sections:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [build-system]            â† PEP 518            â”‚
â”‚  How to build this project                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [project]                 â† PEP 621            â”‚
â”‚  What this project IS (metadata, deps, etc.)    â”‚
â”‚  â”œâ”€ [project.scripts]                           â”‚
â”‚  â”œâ”€ [project.urls]                              â”‚
â”‚  â””â”€ [project.optional-dependencies]             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [tool.*]                  â† Tool-specific      â”‚
â”‚  Configuration for individual tools             â”‚
â”‚  â”œâ”€ [tool.hatch.*]                              â”‚
â”‚  â”œâ”€ [tool.pytest.*]                             â”‚
â”‚  â”œâ”€ [tool.ruff.*]                               â”‚
â”‚  â”œâ”€ [tool.mypy]                                 â”‚
â”‚  â””â”€ [tool.coverage.*]                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Section 1: `[build-system]` (PEP 518)

Tells pip and other installers **how** to build your project.

```toml
[build-system]
requires = ["hatchling"]           # What to download to build
build-backend = "hatchling.build"  # The Python object that does the build
```

| Field | Purpose | Example Values |
|-------|---------|----------------|
| `requires` | Build-time dependencies (downloaded by pip) | `["hatchling"]`, `["setuptools>=68"]`, `["flit_core>=3.9"]` |
| `build-backend` | Python callable that builds sdist/wheel | `"hatchling.build"`, `"setuptools.build_meta"`, `"flit_core.api"` |

**Key insight:** You don't need Hatch installed to `pip install .` your project. pip downloads `hatchling` automatically based on `requires`. Hatch (the CLI) is a separate, optional developer tool.

### Section 2: `[project]` (PEP 621)

Describes **what** your project is. This is standardized metadata â€” every tool reads the same fields.

```toml
[project]
name = "my-project"                  # Package name (PyPI / pip install)
version = "0.1.0"                    # Current version
description = "One-line summary"     # Short description
readme = "README.md"                 # Long description file
requires-python = ">=3.11"           # Minimum Python version
license = {text = "Apache-2.0"}      # SPDX license identifier
authors = [{name = "You"}]           # Author(s)
```

#### Subfields of `[project]`

| Field | Type | Purpose |
|-------|------|---------|
| `name` | string | Package name on PyPI, used with `pip install <name>` |
| `version` | string | SemVer version (or `dynamic = ["version"]` for auto) |
| `description` | string | One-line summary shown on PyPI |
| `readme` | string/table | Path to long description (usually README.md) |
| `requires-python` | string | Minimum Python version specifier |
| `license` | table | SPDX license identifier or file path |
| `authors` | array of tables | Name and/or email of author(s) |
| `keywords` | array of strings | PyPI search keywords |
| `classifiers` | array of strings | [PyPI classifiers](https://pypi.org/classifiers/) (maturity, license, Python versions) |
| `dependencies` | array of strings | **Runtime** dependencies (installed by `pip install .`) |

#### `[project.scripts]` â€” CLI Entry Points

Maps command names to Python functions. pip creates executables for these automatically.

```toml
[project.scripts]
my-tool = "my_package.main:main"     # Runs main() from my_package/main.py
my-tool-doctor = "my_package.main:doctor"
```

After `pip install .`, typing `my-tool` in a terminal calls `my_package.main:main()`.

#### `[project.urls]` â€” Project Links

Shown on the PyPI sidebar.

```toml
[project.urls]
Homepage = "https://github.com/user/project"
Repository = "https://github.com/user/project"
Documentation = "https://project.readthedocs.io"
Changelog = "https://github.com/user/project/blob/main/CHANGELOG.md"
"Bug Tracker" = "https://github.com/user/project/issues"
```

#### `[project.optional-dependencies]` â€” Extra Dependency Groups

Dependencies that are only installed when explicitly requested. This is PEP 621, so **any tool** (pip, Hatch, Dependabot, tox, nox) understands it.

```toml
[project.optional-dependencies]
test = ["pytest", "pytest-cov"]
dev = ["my-project[test]", "ruff", "mypy"]   # Can reference other groups!
docs = ["mkdocs>=1.6", "mkdocs-material>=9.4"]
```

Install with: `pip install -e ".[dev]"` or `pip install -e ".[test,docs]"`.

**Why this matters for Hatch:** Hatch environments reference these groups via `features = ["dev"]` instead of duplicating the dependency list. One source of truth, two consumers.

**Why this matters for Dependabot:** Dependabot reads `[project.optional-dependencies]` and auto-creates PRs to update version specifiers (e.g., `"ruff"` â†’ `"ruff>=0.9.1"`).

### Section 3: `[tool.*]` â€” Tool-Specific Config

Each tool gets its own namespace under `[tool]`. This is not standardized â€” each tool defines its own schema.

```toml
[tool.hatch.envs.default]           # Hatch environment config
[tool.pytest.ini_options]           # pytest settings
[tool.ruff]                         # Ruff linter/formatter
[tool.mypy]                         # mypy type checker
[tool.coverage.run]                 # coverage.py
[tool.bandit]                       # Bandit security linter
```

**Key point:** `[tool.*]` sections are ignored by tools that don't own them. Ruff doesn't care about `[tool.mypy]`, and mypy doesn't care about `[tool.ruff]`. They coexist peacefully.

#### Hatch-Specific Tool Config

Hatch uses `[tool.hatch.*]` for environments, scripts, build config, and versioning:

```toml
# Environments â€” isolated virtualenvs with specific dependency groups
[tool.hatch.envs.default]
features = ["dev"]                     # Install [project.optional-dependencies].dev

[tool.hatch.envs.default.scripts]
test = "pytest {args}"                 # `hatch run test`
lint = "ruff check {args: src/}"       # `hatch run lint`

# Test matrix â€” test across Python versions
[tool.hatch.envs.test]
features = ["test"]

[[tool.hatch.envs.test.matrix]]        # Note: double brackets = array of tables
python = ["3.11", "3.12", "3.13"]

# Build config
[tool.hatch.build.targets.wheel]
packages = ["src/my_package"]
```

### TOML Syntax Quick Reference

TOML has a few syntax patterns that can trip you up:

| Syntax | Meaning | Example |
|--------|---------|---------|
| `[section]` | Table (like a dict) | `[project]` |
| `[section.subsection]` | Nested table | `[tool.ruff.lint]` |
| `[[section]]` | Array of tables (list of dicts) | `[[tool.hatch.envs.test.matrix]]` |
| `key = "value"` | String | `name = "my-project"` |
| `key = ["a", "b"]` | Array | `dependencies = ["click"]` |
| `key = {a = "b"}` | Inline table | `license = {text = "MIT"}` |
| `key = [{a = "b"}]` | Array of inline tables | `authors = [{name = "You"}]` |

### How Tools Discover `pyproject.toml`

Most tools (pytest, ruff, mypy, etc.) walk up the directory tree from the current working directory until they find a `pyproject.toml` with their `[tool.X]` section. No need to pass `--config` flags.

### Putting It All Together

```toml
# â”€â”€ How to build â”€â”€
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# â”€â”€ What this project is â”€â”€
[project]
name = "my-project"
version = "0.1.0"
dependencies = ["requests"]

[project.optional-dependencies]
dev = ["ruff", "pytest"]

[project.scripts]
my-cli = "my_project.main:main"

# â”€â”€ How tools behave â”€â”€
[tool.hatch.envs.default]
features = ["dev"]

[tool.pytest.ini_options]
testpaths = ["tests"]

[tool.ruff]
line-length = 88
```

One file. Everything in one place. Every tool knows where to look.

### Dependabot and `pyproject.toml`

Dependabot understands PEP 621 fully. It scans:

- `[project].dependencies` â€” runtime deps
- `[project.optional-dependencies].*` â€” all extras (dev, test, docs, etc.)

It creates PRs to update version specifiers when new releases are published.

**What Dependabot does NOT do:**

- It does **not** update version numbers inside TOML comments (e.g., `# v0.6.9` in a comment)
- It does **not** update versions in non-standard locations (Taskfile.yml, scripts, README examples)
- It only touches the actual dependency specifier strings

If you have comments documenting current versions (like `"ruff",  # v0.9.1`), those comments will become stale as Dependabot bumps the specifier. You need a separate script or manual process to keep comments current.

---

## Unix, Terminals, and Shells

Before diving into specific shells (bash, zsh, etc.), it helps to understand
the foundational concepts: what Unix is, what a terminal is, and how shells
fit into the picture. These three thingAuto-approve + auto-merge minor/patch Dependabot PRs once CI passes.s are often confused or used
interchangeably, but they're distinct layers.

### What is Unix?

**Unix** is a family of operating systems that originated at AT&T Bell Labs
in 1969 (Ken Thompson, Dennis Ritchie). It introduced ideas that underpin
nearly every modern OS:

- **Everything is a file** â€” devices, sockets, pipes, and actual files are
  all accessed through the same interface (`open`, `read`, `write`, `close`)
- **Small, composable tools** â€” programs that do one thing well and combine
  via pipes (`grep`, `sort`, `awk`, `sed`, `cut`, `wc`)
- **Plain text as a universal interface** â€” configuration, data, and
  inter-process communication default to human-readable text
- **Multi-user, multi-tasking** â€” designed from day one for multiple users
  running multiple programs simultaneously
- **Hierarchical file system** â€” a single root `/` with directories branching
  below it (no drive letters)
- **Permissions model** â€” owner/group/others with read/write/execute bits

#### The Unix Family Tree

```
Unix (AT&T Bell Labs, 1969)
 â”œâ”€â”€ BSD (Berkeley, 1977)
 â”‚    â”œâ”€â”€ FreeBSD
 â”‚    â”œâ”€â”€ OpenBSD
 â”‚    â”œâ”€â”€ NetBSD
 â”‚    â””â”€â”€ macOS / Darwin (Apple, 2001) â† macOS is certified Unix
 â”œâ”€â”€ System V (AT&T, 1983)
 â”‚    â”œâ”€â”€ Solaris (Sun/Oracle)
 â”‚    â”œâ”€â”€ HP-UX
 â”‚    â””â”€â”€ AIX (IBM)
 â””â”€â”€ Linux (Linus Torvalds, 1991) â† "Unix-like", not certified Unix
      â”œâ”€â”€ Debian â†’ Ubuntu, Mint
      â”œâ”€â”€ Red Hat â†’ Fedora, CentOS, RHEL
      â”œâ”€â”€ Arch â†’ Manjaro
      â”œâ”€â”€ Alpine (used in Docker)
      â””â”€â”€ Android (Linux kernel)
```

> **Key distinction:** Linux is *Unix-like* (implements the same concepts and
> mostly follows POSIX standards) but is not descended from AT&T Unix code.
> macOS *is* certified Unix (POSIX-compliant, descended from BSD).

#### POSIX â€” The Compatibility Standard

**POSIX** (Portable Operating System Interface) is a family of standards
published by IEEE (specifically IEEE 1003) and formalised by ISO/IEC. The
name was suggested by Richard Stallman in the late 1980s.

**The problem POSIX solves:** In the 1980s, Unix had fragmented into many
commercial variants â€” AT&T System V, BSD, Sun's SunOS, HP-UX, IBM's AIX.
Each had slightly different system calls, utility flags, shell syntax, and
file layouts. Code written for one often broke on another. POSIX was created
to define a *common baseline* so that software written to the standard would
work on any conforming system.

In plain terms: POSIX is a written specification that says "if you call
yourself a Unix-like operating system, you must support *at least* these
system calls, these shell features, these command-line utilities, and these
behaviors." It's a contract between OS vendors and software developers.

##### What POSIX Actually Defines

| Area | What the standard specifies | Examples |
|------|----------------------------|----------|
| **Shell language** | Syntax, builtins, control flow, variable expansion, quoting rules | `sh` grammar, `if`/`for`/`while`/`case`, `$VAR`, `$(cmd)` |
| **Core utilities** | Required commands and their flags/behavior | `ls`, `cp`, `mv`, `rm`, `grep`, `sed`, `awk`, `find`, `sort`, `test`, `chmod`, `mkdir` |
| **C library API** | System call wrappers and standard functions | `open()`, `read()`, `write()`, `close()`, `fork()`, `exec()`, `pipe()`, `malloc()` |
| **File system** | Path resolution, permissions, symlinks, directory structure | `/`, `/dev`, `/tmp`, permission bits (rwx), `.` and `..` |
| **Environment variables** | Required variables and how they work | `PATH`, `HOME`, `USER`, `SHELL`, `TERM`, `LANG` |
| **Process model** | How processes are created and managed | PIDs, parent/child, signals (`SIGINT`, `SIGTERM`, `SIGKILL`), exit codes, job control |
| **Regular expressions** | Two flavors: Basic (BRE) and Extended (ERE) | BRE for `grep`, ERE for `grep -E` / `egrep` |
| **I/O model** | File descriptors, stdin/stdout/stderr, pipes, redirection | fd 0/1/2, `|`, `>`, `<`, `2>&1` |
| **Threading** | POSIX threads (pthreads) API | `pthread_create()`, `pthread_join()`, mutexes, condition variables |

##### Who Is and Isn't POSIX-Compliant

| System | POSIX status | Notes |
|--------|-------------|-------|
| **macOS** | Certified POSIX-compliant | Apple pays for the certification. macOS is *officially* Unix. |
| **Solaris / illumos** | Certified | Commercial Unix from Sun/Oracle |
| **Linux** | Mostly compliant, not certified | Follows POSIX closely but distros don't pay for certification. In practice, nearly everything works. |
| **FreeBSD / OpenBSD** | Mostly compliant, not certified | BSD heritage, very close to the standard |
| **Windows** | Not POSIX-compliant | Has compatibility layers: WSL (full Linux kernel), Cygwin, MSYS2/Git Bash |
| **Alpine Linux** | POSIX via musl libc | Uses `musl` instead of `glibc`, which is stricter â€” scripts relying on glibc quirks may break |

##### POSIX in Practice â€” What It Means for You

**When writing shell scripts:**

```bash
#!/bin/sh
# POSIX-compliant â€” works everywhere
if [ -f "config.toml" ]; then
    echo "Config found"
fi

# NOT POSIX â€” uses bash-specific [[ ]] syntax
# if [[ -f "config.toml" ]]; then
```

**Common POSIX vs bash differences that bite people:**

| Feature | POSIX `sh` | `bash` |
|---------|-----------|--------|
| Test syntax | `[ -f file ]` | `[[ -f file ]]` (extended, safer) |
| Arrays | Not available | `arr=(a b c)`, `${arr[@]}` |
| String replace | Not available | `${var//old/new}` |
| Process substitution | Not available | `<(command)`, `>(command)` |
| Brace expansion | Not available | `{1..10}`, `{a,b,c}` |
| `source` command | `. file` (dot-space) | `source file` (or `. file`) |
| `function` keyword | `myfunc() { ... }` | `function myfunc() { ... }` (also) |
| `echo` flags | Behavior varies | `-e`, `-n` (but still inconsistent) |
| `local` variables | Not standardised | `local var=value` |

**The practical rule:** Use `#!/bin/sh` and POSIX-only syntax for:
- Git hooks (contributors may use any OS)
- Docker `RUN` commands (Alpine only has `sh`)
- CI scripts that might run on minimal images
- Makefiles (Make defaults to `/bin/sh`)

Use `#!/bin/bash` when you need arrays, `[[ ]]`, string manipulation, or
other bash features â€” and you know bash is available (most Linux distros,
macOS pre-Catalina, CI runners with `bash` specified).

##### Why "POSIX-Compliant" Keeps Coming Up

You'll hear "POSIX" in several contexts:

| Context | What they mean |
|---------|----------------|
| "Write POSIX-compliant scripts" | Use `#!/bin/sh` syntax only â€” no bashisms |
| "POSIX filesystem semantics" | Forward slashes, case-sensitivity, permission bits |
| "POSIX signals" | `SIGINT` (Ctrl+C), `SIGTERM` (graceful stop), `SIGKILL` (force stop) |
| "POSIX threads" (pthreads) | The standard threading API for C/C++ |
| "POSIX regular expressions" | BRE and ERE â€” the regex flavors `grep` and `sed` use |
| "POSIX line endings" | `LF` (`\n`), as opposed to Windows `CRLF` (`\r\n`) |

**Why it matters for this project:** CI runners, Docker containers, and
contributor machines may run different Unix-like systems. Writing
POSIX-compliant scripts (`#!/bin/sh`) maximises portability. Bash-specific
scripts (`#!/bin/bash`) are fine when you know bash is available.

#### Why Unix Matters for Python Development

Even if you develop on Windows, Unix concepts show up everywhere:

| Where | Unix concept |
|-------|-------------|
| **Git** | Built on Unix tools â€” `diff`, `patch`, file permissions, symlinks, line endings (LF vs CRLF) |
| **CI/CD** | GitHub Actions runners are Ubuntu Linux by default |
| **Docker** | Container images are Linux (Alpine, Debian, Ubuntu) |
| **pip / venv** | Virtual environments use Unix-style directory layouts (`bin/`, not `Scripts/` on Linux/macOS) |
| **Shebangs** | `#!/usr/bin/env python3` â€” a Unix convention for executable scripts |
| **File paths** | Forward slashes `/`, case-sensitive names, no drive letters |
| **Package managers** | `apt`, `brew`, `pacman` â€” all Unix-native tools |
| **SSH** | Key-based auth to GitHub, servers â€” a Unix tool (`openssh`) |
| **Permissions** | `chmod +x script.sh` â€” Unix file permission model |
| **Signals** | `Ctrl+C` sends `SIGINT`, `kill -9` sends `SIGKILL` â€” Unix process signals |

#### Unix vs Windows â€” Key Differences

| Concept | Unix / Linux / macOS | Windows |
|---------|---------------------|---------|
| Path separator | `/` (forward slash) | `\` (backslash) |
| Root | `/` | `C:\` (drive letters) |
| Case sensitivity | Case-sensitive (`File.txt` â‰  `file.txt`) | Case-insensitive (usually) |
| Line endings | `LF` (`\n`) | `CRLF` (`\r\n`) |
| Executable marker | File permission bit (`chmod +x`) | File extension (`.exe`, `.bat`, `.ps1`) |
| Shell | `sh`, `bash`, `zsh` | `cmd.exe`, PowerShell |
| Package manager | `apt`, `brew`, `pacman` | `winget`, `choco`, `scoop` |
| Hidden files | Prefix with `.` (`.gitignore`) | File attribute flag |
| Process model | `fork()` + `exec()` | `CreateProcess()` |
| Filesystem | ext4, APFS, ZFS | NTFS |
| User model | Root (`uid 0`) + normal users | Administrator + normal users |

### What is a Terminal?

A **terminal** (or **terminal emulator**) is a program that provides a
text-based window where you type commands and see output. That's it â€” it's
the *window*, not the thing interpreting your commands.

#### Historical Context

```
1960sâ€“70s: Physical terminals (hardware devices with a screen and keyboard)
           â””â”€â”€ VT100, VT220, Teletype (TTY)
                â””â”€â”€ Connected to a mainframe/minicomputer via serial cable

1980sâ€“now: Terminal emulators (software that mimics a physical terminal)
           â””â”€â”€ xterm, GNOME Terminal, iTerm2, Windows Terminal, VS Code terminal
                â””â”€â”€ Connected to a shell process via a pseudo-terminal (PTY)
```

The word "TTY" (teletypewriter) persists in Unix â€” `tty` is a command,
`/dev/tty` is a device file, and terminal-related APIs use the term throughout.

#### Terminal vs Shell vs Command Line

These three terms are often used interchangeably, but they're different layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Terminal Emulator (the window)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Shell (the interpreter)                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Commands / Programs (what you run)      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  e.g., git, python, ls, ruff, pytest     â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Layer | What it is | Examples | Analogy |
|-------|-----------|----------|---------|
| **Terminal** | The window / display surface | Windows Terminal, iTerm2, VS Code integrated terminal, GNOME Terminal | A TV screen |
| **Shell** | The command interpreter that runs inside the terminal | bash, zsh, PowerShell, fish, sh | The channel you're watching |
| **Command** | The program the shell runs | `git commit`, `python main.py`, `ls -la` | The show on the channel |

**Key insight:** You can run *any* shell inside *any* terminal. The terminal
doesn't care â€” it just sends keystrokes to the shell and displays characters
back. You can open Windows Terminal and run bash (via WSL), or open iTerm2 on
macOS and run PowerShell.

#### Common Terminal Emulators

| Terminal | Platform | Key Features |
|----------|----------|-------------|
| **Windows Terminal** | Windows | Tabs, GPU-accelerated, profiles for cmd/PowerShell/WSL |
| **VS Code Integrated Terminal** | Cross-platform | Built into editor, multiple shells, split panes |
| **iTerm2** | macOS | Split panes, hotkey window, search, profiles |
| **GNOME Terminal** | Linux (GNOME) | Default on Ubuntu/Fedora GNOME, tabs, profiles |
| **Alacritty** | Cross-platform | GPU-accelerated, minimal, config-file driven (TOML) |
| **WezTerm** | Cross-platform | GPU-accelerated, Lua config, multiplexer built in |
| **kitty** | Linux / macOS | GPU-accelerated, image display, extensible |
| **Konsole** | Linux (KDE) | KDE default, tabs, profiles, bookmarks |
| **cmd.exe** | Windows | Legacy Windows shell host â€” not really a modern terminal |

#### The VS Code Integrated Terminal

The VS Code terminal is a full terminal emulator embedded in the editor. It
runs a real shell process (bash, zsh, PowerShell, cmd) â€” it's not a
simplified or sandboxed version.

| Feature | Details |
|---------|---------|
| Default shell | Inherits system default (PowerShell on Windows, bash/zsh on Linux/macOS) |
| Switch shells | `Terminal: Select Default Profile` command or dropdown in terminal panel |
| Multiple terminals | Create new ones with `+`, name them, colour-code them |
| Split terminals | Run side-by-side in the same panel |
| Linked to workspace | Working directory defaults to the workspace root |
| Environment | Inherits VS Code's environment variables + activated venv |
| Tasks | Can run registered tasks (`Terminal > Run Task`) |

**Practical tip:** When VS Code activates a Python virtual environment, it
modifies the terminal's `PATH` so `python` and `pip` resolve to the venv's
copies. This is why you see the `(.venv)` prefix in the prompt â€” that's the
shell indicating the venv is active, not the terminal doing it.

### What is a Shell? (Conceptual Overview)

A **shell** is a program that:
1. Displays a prompt
2. Reads a line of input (a command)
3. Parses the command
4. Executes the command (by forking a child process or running a builtin)
5. Displays the output
6. Goes back to step 1

That loop is called a **REPL** (Read-Eval-Print Loop) â€” the same concept as
Python's interactive interpreter (`>>>` prompt).

#### What the Shell Actually Does

Beyond running commands, the shell handles:

| Responsibility | What it does | Example |
|---------------|-------------|---------|
| **Variable expansion** | Replaces `$VAR` with its value | `echo $HOME` â†’ `/home/user` |
| **Glob expansion** | Expands wildcards into matching filenames | `ls *.py` â†’ `ls main.py utils.py` |
| **Pipes** | Connects stdout of one command to stdin of the next | `cat log.txt \| grep ERROR \| wc -l` |
| **Redirection** | Sends output to a file or reads input from a file | `echo "hello" > out.txt` |
| **Job control** | Runs processes in background, foreground, suspend | `sleep 100 &`, `fg`, `Ctrl+Z` |
| **Environment** | Maintains environment variables passed to child processes | `export PATH="$PATH:/usr/local/bin"` |
| **Scripting** | Conditionals, loops, functions â€” it's a programming language | `if [ -f .env ]; then source .env; fi` |
| **History** | Remembers previous commands (arrow keys, `Ctrl+R` search) | `history`, `!!` (rerun last command) |
| **Tab completion** | Completes filenames, commands, arguments | Type `git com` + Tab â†’ `git commit` |
| **Signal handling** | Catches `Ctrl+C` (SIGINT), `Ctrl+D` (EOF), etc. | `trap 'cleanup' EXIT` |

#### Interactive vs Non-Interactive Shells

| Mode | When | Config loaded | Use case |
|------|------|--------------|----------|
| **Interactive login** | SSH, first terminal after boot | `.bash_profile` (or `.zprofile`) then `.bashrc` | User's main session |
| **Interactive non-login** | Open a new terminal tab | `.bashrc` (or `.zshrc`) | Daily use |
| **Non-interactive** | Running a script (`bash script.sh`) | Usually none (or `BASH_ENV` if set) | Automation, CI, cron |

**Why this matters:** If you set an alias in `.bashrc` but your CI script runs
non-interactively, that alias won't exist. Environment setup for scripts
should go in the script itself or be passed explicitly.

#### How the Shell Runs a Command

When you type `python main.py` and press Enter, here's what happens:

```
1. Shell reads the line: "python main.py"
2. Shell parses it: command="python", args=["main.py"]
3. Shell searches $PATH for "python" executable
   â†’ Finds /usr/bin/python (or .venv/bin/python if venv active)
4. Shell calls fork() â†’ creates a child process
5. Child process calls exec("python", ["main.py"])
   â†’ Child process is replaced by the Python interpreter
6. Python runs main.py
7. Python exits with exit code (0 = success, non-zero = error)
8. Shell receives the exit code â†’ stores in $?
9. Shell prints the next prompt
```

This `fork + exec` model is fundamental to Unix. Every command you run
(except shell builtins like `cd`, `echo`, `export`) goes through this cycle.

**Builtins are special:** Commands like `cd`, `export`, `source`, and `alias`
must run *inside* the shell process (not in a child) because they modify the
shell's own state. `cd` changes the shell's working directory â€” if it ran as
a child process, only the child would change directories, and the parent shell
would be unaffected.

---

## Raw SQL vs ORMs in Python

When people say "raw SQL" they mean writing SQL statements directly as
strings in your code, as opposed to using an abstraction layer that generates
SQL for you. Both approaches talk to the same database â€” the difference is
*who writes the SQL*: you, or a library.

### What "Raw SQL" Actually Means

**Raw SQL** = you write the SQL yourself as a literal string, send it to the
database, and handle the results.

```python
import sqlite3

# This is raw SQL â€” you wrote the SELECT statement yourself
conn = sqlite3.connect("app.sqlite3")
cursor = conn.execute(
    "SELECT id, name, email FROM users WHERE active = ? ORDER BY name",
    (True,)
)
for row in cursor:
    print(row[0], row[1], row[2])  # access by index â€” no named attributes
conn.close()
```

**ORM (Object-Relational Mapper)** = a library translates Python
objects/method calls into SQL behind the scenes.

```python
from sqlalchemy.orm import Session
from models import User

# This is ORM â€” SQLAlchemy generates the SQL for you
with Session() as session:
    users = (
        session.query(User)
        .filter(User.active == True)
        .order_by(User.name)
        .all()
    )
    for user in users:
        print(user.id, user.name, user.email)  # named attributes on objects
```

Both produce the same `SELECT id, name, email FROM users WHERE active = 1
ORDER BY name` query. The ORM just writes it for you.

### The Spectrum: It's Not Binary

It's not just "raw SQL" vs "full ORM" â€” there's a spectrum:

| Level | Approach | Library Examples | You Write SQL? |
|-------|---------|-----------------|----------------|
| **1. Raw SQL** | Strings + database driver | `sqlite3`, `psycopg2`, `mysql-connector` | Yes â€” full SQL |
| **2. SQL builder / query builder** | Python objects that compose SQL pieces | `pypika`, `sqlbuilder` | Partially â€” Python API, SQL output |
| **3. Core SQL toolkit** | Expression language that maps closely to SQL | SQLAlchemy Core, `databases` | Sort of â€” SQL-like Python expressions |
| **4. Lightweight ORM** | Thin models, minimal magic | Peewee, PonyORM, SQLModel | No â€” but you see the SQL shape |
| **5. Full ORM** | Models, relationships, identity map, unit of work | SQLAlchemy ORM, Django ORM, Tortoise | No â€” heavily abstracted |

Many experienced developers land at levels 2â€“3: they want composable queries
without the overhead and complexity of a full ORM.

### Why Many Python Projects Use ORMs Instead of Raw SQL

You're right that SQL is powerful and useful â€” it absolutely is. SQL has been
around since the 1970s, is standardised (like POSIX for databases), and is
the most widely used language for data. The reason many Python projects reach
for ORMs isn't that SQL is bad â€” it's about managing complexity at scale:

**1. Boilerplate and repetition**

CRUD operations (Create, Read, Update, Delete) are repetitive in raw SQL.
For every table you need INSERT, SELECT, UPDATE, DELETE statements, parameterized
correctly, with result-set parsing. ORMs generate all of this from a model
definition.

```python
# Raw SQL: 4 separate statements to write and maintain per table
INSERT_USER = "INSERT INTO users (name, email) VALUES (?, ?)"
SELECT_USER = "SELECT id, name, email FROM users WHERE id = ?"
UPDATE_USER = "UPDATE users SET name = ?, email = ? WHERE id = ?"
DELETE_USER = "DELETE FROM users WHERE id = ?"

# ORM: one model definition handles all CRUD
class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True)
    name = Column(String, nullable=False)
    email = Column(String, unique=True)
```

**2. SQL injection risk**

Raw SQL makes it easy to accidentally interpolate user input into queries
(especially for beginners). ORMs parameterize automatically.

```python
# DANGEROUS â€” SQL injection vulnerability
cursor.execute(f"SELECT * FROM users WHERE name = '{user_input}'")
# If user_input = "'; DROP TABLE users; --"  ...goodbye data

# SAFE â€” parameterized query (raw SQL done properly)
cursor.execute("SELECT * FROM users WHERE name = ?", (user_input,))

# SAFE â€” ORM handles parameterization
session.query(User).filter(User.name == user_input).all()
```

**3. Schema â†” code synchronization**

With raw SQL, your database schema and your Python code are two separate
things that can drift apart. If you add a column to the database, nothing in
your Python code knows about it until you manually update your queries. ORMs
keep the schema definition *in* the Python code, often with migration tools
that auto-detect changes.

**4. Relationships and lazy loading**

Navigating relationships between tables ("get this user's orders, then each
order's items") requires joins or multiple queries in raw SQL. ORMs let you
traverse relationships like Python attributes:

```python
# Raw SQL: manual join
cursor.execute("""
    SELECT u.name, o.total, i.product_name
    FROM users u
    JOIN orders o ON o.user_id = u.id
    JOIN order_items i ON i.order_id = o.id
    WHERE u.id = ?
""", (user_id,))

# ORM: traverse like Python objects
user = session.get(User, user_id)
for order in user.orders:           # lazy-loads orders
    for item in order.items:        # lazy-loads items
        print(item.product_name)
```

**5. Database portability**

Raw SQL is often dialect-specific. PostgreSQL, MySQL, and SQLite have
different syntax for things like auto-increment, string functions, date
handling, and `UPSERT`. ORMs abstract these differences â€” switch your
connection string and (mostly) the same code works on a different database.

**6. Web framework integration**

The biggest Python web frameworks ship with ORMs built in or strongly
recommended:
- **Django** â†’ Django ORM (built in, tightly integrated)
- **Flask** â†’ SQLAlchemy (via Flask-SQLAlchemy)
- **FastAPI** â†’ SQLAlchemy or SQLModel

Since most Python web tutorials start with these frameworks, new developers
learn ORMs first and may never write raw SQL in Python.

### When Raw SQL Is the Better Choice

Despite the above, there are solid reasons to use raw SQL:

| Scenario | Why raw SQL wins |
|----------|------------------|
| **Complex queries** | Multi-table joins, window functions, CTEs, recursive queries â€” ORMs struggle with these or produce inefficient SQL |
| **Performance-critical paths** | You know exactly what query runs, no ORM overhead or N+1 surprises |
| **Reporting / analytics** | Aggregations, GROUP BY, HAVING â€” often cleaner in SQL |
| **Database-specific features** | Full-text search, JSON operators, PostGIS, SQLite FTS5 â€” ORMs may not expose these |
| **Simple scripts** | A 50-line script doesn't need an ORM setup |
| **Learning** | Understanding SQL directly makes you a better developer, even if you later use an ORM |
| **Existing schema** | Working with a database you didn't design â€” raw SQL adapts easier than mapping an ORM |
| **Data migrations** | Schema changes, backfills, one-off fixes â€” raw SQL is the right tool |

### The ORM Drawbacks People Don't Mention Upfront

| Problem | What happens |
|---------|-------------|
| **N+1 queries** | ORM lazy-loads related objects one at a time â€” 100 users with orders = 101 queries instead of 1 join |
| **Opaque SQL** | Hard to see what SQL the ORM generates; performance debugging requires logging SQL output |
| **Migration complexity** | ORM migration tools (Alembic, Django migrations) can generate incorrect or inefficient migrations |
| **Learning the ORM â‰  learning SQL** | ORMs have their own API, quirks, and mental model â€” you're learning *the ORM*, not *databases* |
| **Abstraction leaks** | Eventually you hit something the ORM can't do and drop to raw SQL anyway |
| **Heavyweight** | SQLAlchemy is ~45k lines of code. For a script that runs 3 queries, that's a lot of machinery |

### What This Project Does

This template uses the `db/` directory with raw SQL files:
- `db/schema.sql` â€” full schema definition
- `db/migrations/` â€” incremental changes as numbered `.sql` files
- `db/seeds/` â€” test/dev data
- `db/queries/` â€” reusable query snippets

This is the **raw SQL** approach. The template doesn't include an ORM because:
1. It's a *template* â€” template users choose their own data layer
2. Not every Python project needs a database at all
3. Raw `.sql` files are database-agnostic in structure (even if the SQL
   dialect varies)
4. It keeps the template dependency-free for database concerns

If you add a database to a project based on this template, you'd choose:
- **Raw SQL** (`sqlite3` / `psycopg2`) for simple cases or when you want full control
- **SQLAlchemy Core** for composable queries without full ORM overhead
- **SQLAlchemy ORM** / **Django ORM** for web apps with lots of CRUD
- **SQLModel** for FastAPI projects (combines SQLAlchemy + Pydantic)

### Python Database Libraries at a Glance

| Library | Type | Database | When to use |
|---------|------|----------|-------------|
| **sqlite3** | Raw SQL (stdlib) | SQLite | Scripts, prototypes, single-user apps, testing |
| **psycopg2** / **psycopg3** | Raw SQL driver | PostgreSQL | Direct Postgres access, performance-critical |
| **mysql-connector** / **PyMySQL** | Raw SQL driver | MySQL/MariaDB | Direct MySQL access |
| **SQLAlchemy Core** | SQL toolkit | Any (via dialects) | Composable queries, multi-DB support |
| **SQLAlchemy ORM** | Full ORM | Any (via dialects) | Web apps, complex domain models |
| **Django ORM** | Full ORM (Django only) | PostgreSQL, MySQL, SQLite | Django projects |
| **Peewee** | Lightweight ORM | SQLite, PostgreSQL, MySQL | Small projects, scripts |
| **SQLModel** | ORM + validation | Any (SQLAlchemy backend) | FastAPI projects |
| **Tortoise ORM** | Async ORM | PostgreSQL, MySQL, SQLite | Async web apps |
| **databases** | Async raw SQL | PostgreSQL, MySQL, SQLite | Async apps with raw queries |

### The Pragmatic Take

SQL itself is one of the most valuable skills you can learn as a developer.
It's been around for 50 years and isn't going anywhere. The question isn't
"raw SQL vs ORM" â€” it's *where in the spectrum do you want to operate for
this particular project*.

Many experienced developers:
1. **Learn SQL properly first** â€” understand SELECT, JOIN, GROUP BY, window
   functions, indexing, query plans
2. **Use an ORM/toolkit for application code** â€” reduces boilerplate, handles
   the boring CRUD
3. **Drop to raw SQL when needed** â€” complex reports, performance-sensitive
   queries, migrations, data fixes

The worst outcome is learning only the ORM and not understanding what it
generates. If you can write the SQL yourself, you can evaluate whether the
ORM is doing something sensible. If you can't, you're flying blind.

### Setting Up SQL CI and Hooks

If you add real SQL to a project using this template, you'll want automated
checks to catch issues early. The exact setup depends on which approach you
choose (raw SQL, ORM, or somewhere in between).

#### Option 1: Raw SQL Files (what this template's `db/` directory supports)

If you keep schema, migrations, and queries as `.sql` files:

| Check | Tool | Where to run | What it catches |
|-------|------|--------------|-----------------|
| **Syntax validation** | `sqlite3 :memory: < db/schema.sql` | CI workflow, pre-commit hook | Malformed SQL that won't parse |
| **Lint + format** | [SQLFluff](https://sqlfluff.com/) | CI workflow, pre-commit hook | Style violations, anti-patterns, inconsistent formatting |
| **Migration order** | Custom script (`scripts/`) | CI workflow | Duplicate or out-of-order migration numbers |
| **Migration apply** | Apply migrations sequentially to empty DB | CI workflow | Migrations that fail, conflict, or don't compose |
| **Seed data** | Apply seeds after schema | CI workflow | Seeds that violate constraints |

**Pre-commit hook example (SQLFluff):**

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/sqlfluff/sqlfluff
    rev: 3.3.1  # check for latest
    hooks:
      - id: sqlfluff-lint
        args: [--dialect, sqlite]  # or postgres, mysql, etc.
        files: \.sql$
      - id: sqlfluff-fix
        args: [--dialect, sqlite]
        files: \.sql$
```

**CI workflow example (schema validation):**

```yaml
# .github/workflows/sql-check.yml
name: SQL Check
on: [push, pull_request]
jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@...
      - name: Validate schema
        run: sqlite3 :memory: < db/schema.sql
      - name: Apply migrations in order
        run: |
          sqlite3 :memory: < db/schema.sql
          for f in db/migrations/*.sql; do
            echo "Applying $f"
            sqlite3 :memory: < "$f" || exit 1
          done
```

#### Option 2: ORM (SQLAlchemy, Django ORM, etc.)

If you use an ORM, SQL validation happens differently:

| Check | Tool | What it catches |
|-------|------|-----------------|
| **Model validation** | pytest + ORM setup | Models that don't map to valid schema |
| **Migration generation** | `alembic check` / `python manage.py makemigrations --check` | Missing migrations |
| **Migration apply** | `alembic upgrade head` against a test DB | Migrations that fail |
| **Integration tests** | pytest with a test database | Queries that fail at runtime |

ORMs handle SQL generation, so you lint Python code (Ruff, mypy) rather than
SQL files. But you should still test that migrations apply cleanly and that
your models match the actual database.

#### Option 3: Hybrid (ORM + raw SQL for complex queries)

Many projects use an ORM for CRUD and drop to raw SQL for complex queries,
reports, or performance-critical paths. In that case, combine both approaches:
- Lint `.sql` files with SQLFluff
- Test ORM models and migrations with pytest
- Integration tests that exercise both code paths

#### What to Start With

For a new project using this template:
1. **Immediately:** Add a `task db:check` shortcut that runs
   `sqlite3 :memory: < db/schema.sql` â€” zero dependencies, instant sanity check
2. **When you have real SQL files:** Add SQLFluff as a pre-commit hook
3. **When you have migrations:** Add a CI job that applies them sequentially
4. **When you have data access code:** Add integration tests with a test database

The key principle: validate the SQL layer the same way you validate Python
code. If it can break, it should have a check.

---

## Shells: sh, bash, zsh, and Others

Shells are command-line interpreters â€” programs that read your commands and execute them. They matter for git hooks, scripts, CI pipelines, and daily terminal use. Each shell is a superset or variant of the one before it.

### The Shell Family Tree

```
sh (Bourne Shell, 1979)
 â””â”€â”€ bash (Bourne Again Shell, 1989)
      â””â”€â”€ zsh (Z Shell, 1990)

csh (C Shell, 1978)         â† separate lineage
 â””â”€â”€ tcsh
      â””â”€â”€ fish (2005)       â† inspired by csh, but independent
```

### Shell Comparison

| Shell | Full Name | Default On | Best For | Key Trait |
|-------|-----------|-----------|----------|-----------|
| **sh** | Bourne Shell | POSIX systems, Docker `alpine` | Portable scripts, git hooks, CI | Minimal â€” works everywhere |
| **bash** | Bourne Again Shell | Most Linux distros, older macOS | General scripting, interactive use | Arrays, `[[ ]]`, `$()`, rich scripting |
| **zsh** | Z Shell | macOS (since Catalina), many devs | Interactive daily use | Plugins (Oh My Zsh), autocomplete, glob |
| **dash** | Debian Almquist Shell | Debian/Ubuntu (`/bin/sh` â†’ dash) | System scripts | Extremely fast, strict POSIX |
| **fish** | Friendly Interactive Shell | â€” (opt-in) | Interactive use, beginners | Syntax highlighting, autosuggestions |
| **PowerShell** | PowerShell | Windows | Windows automation, .NET | Object pipeline (not text), cmdlets |

### sh â€” The Portable Baseline

`sh` (POSIX shell) is the lowest common denominator. If you write a script in `sh`, it will run on virtually any Unix-like system â€” Linux, macOS, BSD, Docker containers, CI runners.

```bash
#!/bin/sh
# This runs everywhere. No bashisms allowed.
echo "Hello from sh"
```

**Key limitations** (things sh does NOT have):
- No arrays (`arr=(a b c)` is bash)
- No `[[ ]]` (use `[ ]` instead)
- No `$(( ))` for arithmetic in all implementations
- No `{1..10}` brace expansion
- No `function` keyword (just `myfunc() { ... }`)

**Why this matters:** On Debian/Ubuntu, `/bin/sh` is actually `dash` (not bash), so scripts with `#!/bin/sh` that use bash features will silently break.

### bash â€” The Workhorse

Bash is the most widely used shell for scripting. It extends sh with arrays, better string manipulation, `[[ ]]` tests, process substitution, and more.

```bash
#!/bin/bash
# Bash-specific features
names=("alice" "bob" "charlie")      # arrays
for name in "${names[@]}"; do
    if [[ "$name" == a* ]]; then     # [[ ]] pattern matching
        echo "Found: $name"
    fi
done
```

**Bash vs sh â€” common "bashisms" that break in sh:**

| Feature | bash | sh (POSIX) |
|---------|------|-----------|
| Test syntax | `[[ -f file ]]` | `[ -f file ]` |
| Arrays | `arr=(a b c)` | Not available |
| String substitution | `${var//old/new}` | Not available |
| Process substitution | `<(command)` | Not available |
| Brace expansion | `{1..5}` | Not available |
| `source` command | `source file` | `. file` |
| Function keyword | `function foo()` | `foo()` |

### zsh â€” The Interactive Powerhouse

zsh is bash-compatible for most scripting but shines as an interactive shell with better tab completion, theming, spelling correction, and plugin ecosystems like **Oh My Zsh**.

```zsh
#!/bin/zsh
# zsh-specific features
typeset -A config                   # associative arrays (bash 4+ also has these)
config[host]=localhost
config[port]=8080
echo "Server: $config[host]:$config[port]"

# Glob qualifiers â€” zsh-only
print -l *.py(om)                   # list .py files sorted by modification time
```

**Why macOS switched to zsh:** Apple shipped bash 3.2 (2007) because bash 4+ is GPLv3, which conflicts with Apple's licensing. Rather than ship ancient bash, they switched the default to zsh (MIT-licensed) in macOS Catalina (2019).

### Which Shell for What?

| Use Case | Recommended Shell | Why |
|----------|------------------|-----|
| **Git hooks** | `#!/bin/sh` | Portability â€” hooks must work on every contributor's machine |
| **CI/CD scripts** | `#!/bin/sh` or `#!/bin/bash` | CI runners have bash, but sh is safer for Docker alpine |
| **Complex automation scripts** | `#!/bin/bash` | Need arrays, string ops, or conditionals |
| **Daily terminal use** | zsh or fish | Better autocomplete, history, plugins |
| **Makefiles / Taskfiles** | sh (implicit) | Make uses `/bin/sh` by default |
| **Docker `RUN` commands** | sh | Alpine images only have sh, not bash |
| **Windows scripts** | PowerShell | Native, object-based pipeline |

### Shells and Git Hooks

Git hooks are executable scripts in `.git/hooks/`. The shebang line (`#!/bin/sh`) determines which shell interprets them.

```bash
#!/bin/sh
# .git/hooks/pre-commit â€” runs before every commit
# Using sh for maximum portability

echo "Running pre-commit checks..."
python -m ruff check src/ || exit 1
```

**Why pre-commit (the framework) helps:** Instead of writing raw shell hook scripts, `pre-commit` manages hooks via `.pre-commit-config.yaml`. It handles shebang lines, virtual environments, and cross-platform compatibility â€” you never need to think about which shell the hook uses.

**Without pre-commit** (raw hooks):
- You write shell scripts directly in `.git/hooks/`
- You choose the shell (`#!/bin/sh`, `#!/bin/bash`, etc.)
- You handle portability yourself
- Hooks aren't versioned (`.git/hooks/` is not committed)

**With pre-commit** (framework):
- Hooks are defined in `.pre-commit-config.yaml` (versioned)
- The framework generates the actual hook scripts
- Each hook tool runs in its own isolated environment
- Shell portability is handled for you

### Common Gotcha: Shebang Lines

The shebang (`#!`) must be the **first line** of the script, with no leading whitespace or BOM:

```bash
#!/bin/sh          â† correct
#!/bin/bash        â† correct, but limits portability
#!/usr/bin/env bash  â† most portable way to invoke bash (finds it in $PATH)
```

`#!/usr/bin/env bash` is preferred over `#!/bin/bash` because bash isn't always at `/bin/bash` (e.g., on NixOS or some BSD systems). `env` searches `$PATH` to find it.

### Hook Scripts in Other Programming Languages

Git hooks don't have to be shell scripts. Any executable file with a valid shebang line works. This opens the door to Python, Node.js, Ruby, Perl, Rust, Go â€” whatever you have installed.

#### Language Examples for Hooks

**Python:**
```python
#!/usr/bin/env python3
"""pre-commit hook: check for TODO comments with no issue reference."""
import subprocess
import sys

result = subprocess.run(
    ["git", "diff", "--cached", "--name-only", "--diff-filter=ACM"],
    capture_output=True, text=True
)
staged_files = result.stdout.strip().splitlines()

for filepath in staged_files:
    with open(filepath) as f:
        for i, line in enumerate(f, 1):
            if "TODO" in line and "#" not in line.split("TODO")[1][:10]:
                print(f"{filepath}:{i}: TODO without issue reference")
                sys.exit(1)
```

**Node.js:**
```javascript
#!/usr/bin/env node
// pre-commit hook: validate JSON files
const fs = require('fs');
const { execSync } = require('child_process');

const staged = execSync('git diff --cached --name-only --diff-filter=ACM')
  .toString().trim().split('\n')
  .filter(f => f.endsWith('.json'));

let failed = false;
for (const file of staged) {
  try {
    JSON.parse(fs.readFileSync(file, 'utf8'));
  } catch (e) {
    console.error(`Invalid JSON: ${file} â€” ${e.message}`);
    failed = true;
  }
}
process.exit(failed ? 1 : 0);
```

**Ruby:**
```ruby
#!/usr/bin/env ruby
# pre-commit hook: check for binding.pry left in code
staged = `git diff --cached --name-only --diff-filter=ACM`.split("\n")
staged.select { |f| f.end_with?('.rb') }.each do |file|
  File.readlines(file).each_with_index do |line, i|
    if line.include?('binding.pry')
      puts "#{file}:#{i + 1}: Remove binding.pry before committing"
      exit 1
    end
  end
end
```

**Perl:**
```perl
#!/usr/bin/env perl
# pre-commit hook: check for trailing whitespace
use strict;
my @files = `git diff --cached --name-only --diff-filter=ACM`;
chomp @files;
for my $file (@files) {
    open my $fh, '<', $file or next;
    while (<$fh>) {
        if (/\s+$/) {
            print "$file:$.: trailing whitespace\n";
            exit 1;
        }
    }
}
```

#### Compiled Languages as Hooks

Compiled languages (Rust, Go, C) can also be used â€” you compile the binary first, then point the hook at it. This is less common for one-off hooks but used by dedicated hook tools:

```bash
#!/bin/sh
# Hook that delegates to a compiled Go binary
exec .git/hooks/bin/my-hook "$@"
```

Notable tools written in compiled languages that serve as hook systems:
- **lefthook** (Go) â€” fast, parallel hook runner with YAML config
- **rusty-hook** (Rust) â€” lightweight hook runner for Node projects
- **overcommit** (Ruby) â€” full-featured hook manager

#### Shells vs Programming Languages for Hooks

| Factor | Shell (sh/bash) | Python | Node.js | Compiled (Go/Rust) |
|--------|----------------|--------|---------|-------------------|
| **Startup speed** | Instant (~5ms) | Slow (~50-100ms) | Slow (~100ms) | Instant (~5ms) |
| **Portability** | sh is everywhere | Needs Python installed | Needs Node installed | Binary runs anywhere |
| **String/text processing** | Awkward (sed, awk, grep) | Excellent | Good | Good |
| **Error handling** | Fragile (`set -e`, exit codes) | try/except, robust | try/catch, robust | Strong type system |
| **File system operations** | Basic (test, find, ls) | `pathlib`, `os` â€” powerful | `fs` module â€” decent | Full stdlib |
| **JSON/YAML parsing** | Needs `jq` or similar | Built-in `json` module | Built-in `JSON` | Serde (Rust), encoding/json |
| **Git integration** | Native (`git` commands) | Subprocess calls | Subprocess or libraries | Subprocess or `git2` |
| **Complexity ceiling** | Low (~50 lines max) | Unlimited | Unlimited | Unlimited |
| **Dependencies** | None | pip/venv | npm/node_modules | Compile step |
| **Debugging** | Painful (`set -x`) | Proper debugger (pdb) | Proper debugger | Proper debugger |
| **Windows support** | Needs Git Bash/WSL | Native | Native | Native |

#### When to Use What

| Scenario | Best Choice | Why |
|----------|------------|-----|
| Simple file checks (whitespace, merge markers) | **Shell (sh)** | 2-5 lines, no dependencies, instant |
| Check staged file contents or patterns | **Python** | Easy file I/O, regex, readable |
| Validate JSON/YAML/config files | **Python or Node** | Built-in parsers |
| Complex multi-step validation | **Python** | Best balance of power and readability |
| Enforce commit message format | **Shell or Python** | Shell for simple regex, Python for complex rules |
| Performance-critical (huge repos) | **Compiled (Go/Rust)** | Sub-millisecond execution |
| Team with mixed OS (Windows + Mac + Linux) | **Python or Node** | Cross-platform without shell quirks |
| You already use pre-commit framework | **Doesn't matter** | pre-commit abstracts the language away |

#### The Reality: pre-commit Framework Handles This

In practice, choosing a language for hooks is mostly academic if you use the `pre-commit` framework (which this project does). Each hook in `.pre-commit-config.yaml` runs its own tool in an isolated environment:

```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit  # Rust binary
    hooks:
      - id: ruff           # â† you don't care that Ruff is written in Rust
  - repo: https://github.com/pre-commit/mirrors-mypy     # Python tool
    hooks:
      - id: mypy           # â† you don't care that mypy is Python
  - repo: https://github.com/pre-commit/pre-commit-hooks # Python scripts
    hooks:
      - id: check-yaml     # â† you don't care about the implementation
```

The framework:
- Downloads and installs each hook's dependencies automatically
- Creates isolated environments (virtualenvs for Python, node_modules for Node, etc.)
- Handles shebang lines and shell compatibility
- Works identically on macOS, Linux, and Windows

**Bottom line:** The pre-commit framework lets you use the *best tool for the job* regardless of what language it's written in. You pick hooks by *what they check*, not *what language they use*.

### Quick Reference: Shell Config Files

| Shell | Login Shell | Interactive (non-login) | Notes |
|-------|------------|------------------------|-------|
| **bash** | `~/.bash_profile` or `~/.profile` | `~/.bashrc` | `.bash_profile` often sources `.bashrc` |
| **zsh** | `~/.zprofile` then `~/.zshrc` | `~/.zshrc` | Oh My Zsh configures this |
| **sh** | `~/.profile` | â€” | Minimal config |
| **fish** | `~/.config/fish/config.fish` | Same file | No login/non-login split |

---

## Repo Versioning â€” Manual vs Automatic

Every repo needs a version number, but *where* that number lives and *how* it gets updated varies. This is the fundamental decision that shapes your release workflow.

### The Core Question: Who Decides the Version?

| Approach | Who/what sets the version | Where the version lives | When it changes |
|----------|--------------------------|------------------------|-----------------|
| **Manual** | Developer edits a file | Hardcoded in source | When you remember to update it |
| **Semi-automatic** | Developer triggers a tool | Tool updates source file(s) | When you run the bump command |
| **Fully automatic** | CI derives from commits/tags | Git tags or computed at build time | Every qualifying merge to main |

### Manual Versioning

You write the version string directly in one or more files and update it by hand before each release.

#### Where the version can live

```toml
# pyproject.toml â€” static version
[project]
version = "1.2.3"
```

```python
# src/my_package/__init__.py
__version__ = "1.2.3"
```

```python
# src/my_package/_version.py (dedicated version file)
VERSION = "1.2.3"
```

#### Typical manual workflow

```bash
# 1. Edit pyproject.toml (and any other files with version strings)
# 2. Commit
git add pyproject.toml
git commit -m "chore: bump version to 1.3.0"
# 3. Tag
git tag v1.3.0
# 4. Push
git push origin main --tags
```

#### Problems with manual versioning

- **Drift** â€” easy to update pyproject.toml but forget `__init__.py` or vice versa
- **Human error** â€” typos, skipped versions, forgetting to tag
- **No changelog** â€” you have to write release notes from memory
- **Merge conflicts** â€” version bumps in pyproject.toml create conflicts between parallel PRs
- **Tag/version mismatch** â€” commit says `1.3.0` but you tagged `v1.2.9`

#### When manual versioning is fine

- Solo projects with infrequent releases
- Learning how versioning works (do it manually first, then automate)
- Projects with no consumers (internal tools, scripts)
- Very early development where releases don't matter yet

### Automatic Versioning

The version is *derived* â€” either from git tags at build time, or from commit messages by a CI tool.

#### Approach A: Tag-derived (build-time versioning)

The version doesn't exist in any source file. Instead, a build plugin reads the latest git tag and computes the version.

```toml
# pyproject.toml â€” dynamic version via hatch-vcs
[project]
dynamic = ["version"]

[tool.hatch.version]
source = "vcs"         # version comes from git tags

[tool.hatch.build.hooks.vcs]
version-file = "src/my_package/_version.py"  # generated at build time
```

**How it works:**

```text
git tag v1.2.0 on commit abc123

After tagging:
  pip install .  â†’  version = "1.2.0"

3 commits later (no new tag):
  pip install .  â†’  version = "1.2.0.dev3+g7f8e9a1"
                              â†‘ 3 commits since tag, at this hash
```

**Tools that do this:**

| Tool | Build backend | Config |
|------|--------------|--------|
| **hatch-vcs** | Hatchling | `[tool.hatch.version] source = "vcs"` |
| **setuptools-scm** | Setuptools | `[tool.setuptools_scm]` |
| **versioningit** | Any (Hatchling, setuptools, etc.) | `[tool.versioningit]` |
| **dunamai** | Any (library/CLI) | CLI flags or API calls |

**Pros:**

- Zero maintenance â€” version is always correct
- No merge conflicts â€” no version string in source files
- Tag is the single source of truth â€” impossible for code and tag to drift
- Dev versions (`1.2.0.dev3`) are automatic for unreleased commits

**Cons:**

- Requires git history at build time (`git clone --depth 1` breaks it)
- Can be confusing â€” "where is the version?" has no obvious answer
- Import-time overhead if the version is computed dynamically (vs generated file)
- CI must have full git history or at least tags (`fetch-depth: 0`)

#### Approach B: Commit-derived (CI determines the bump)

A CI tool reads commit messages (conventional commits), determines the bump type, updates the version, and creates the release â€” all automatically.

```text
feat: add export endpoint        â†’  CI bumps minor:  1.2.0 â†’ 1.3.0
fix: handle null email           â†’  CI bumps patch:  1.3.0 â†’ 1.3.1
feat!: redesign auth API         â†’  CI bumps major:  1.3.1 â†’ 2.0.0
chore: update deps               â†’  no release
```

**Tools that do this:**

| Tool | How it manages versions |
|------|------------------------|
| **release-please** | Opens a Release PR tracking pending changes; merging bumps version, updates CHANGELOG, creates GitHub Release + tag |
| **python-semantic-release** | Runs in CI, parses commits, bumps version in source, tags, publishes to PyPI |
| **commitizen** | `cz bump` reads commits and bumps version; can run locally or in CI |
| **semantic-release** (JS) | The original Node.js version â€” full plugin pipeline |

**Pros:**

- Fully hands-off â€” merge PRs with good commit messages, releases happen
- Changelog is generated automatically from commit history
- Version bumps are deterministic â€” same commits always produce same version
- Enforces commit discipline (teams must write meaningful commit messages)

**Cons:**

- Requires disciplined commit messages â€” messy commits = wrong versions
- Opinionated â€” you give up control over when releases happen
- Debugging release issues means reading CI logs, not local files
- Learning curve for the tooling configuration

### Combining Both Approaches (What This Project Does)

This project uses **tag-derived versioning** (hatch-vcs) for the package version and **commit-derived releases** (release-please) for deciding *when* to create tags:

```text
Developer writes conventional commits
  â†’ release-please opens a Release PR (accumulates changes)
  â†’ Merging the Release PR creates a git tag (e.g. v1.3.0)
  â†’ hatch-vcs reads that tag at build time â†’ package version = 1.3.0
  â†’ GitHub Actions builds and publishes artifacts
```

This gives you:

- **No version in source code** â€” hatch-vcs derives it from tags
- **Automatic release timing** â€” release-please decides when to release based on commits
- **Human review** â€” the Release PR lets you review the changelog before merging
- **Correct versions everywhere** â€” tag, package metadata, and CHANGELOG all agree

### Decision Matrix: Which Approach to Choose

| Factor | Manual | Semi-auto (bump tools) | Tag-derived | Commit-derived | Both (this project) |
|--------|--------|----------------------|-------------|----------------|---------------------|
| **Effort per release** | High | Medium | None | None | None |
| **Risk of version drift** | High | Medium | None | Low | None |
| **Changelog** | Manual | Manual | Manual | Automatic | Automatic |
| **Commit discipline needed** | No | No | No | Yes | Yes |
| **Setup complexity** | None | Low | Low | Medium | Medium |
| **Best for** | Learning, solo | Small teams | Libraries | Apps, teams | Template repos, mature projects |

See also: [Release Workflows](#release-workflows) below for the full tool comparison, and [ADR 021](../adr/021-automated-release-pipeline.md) for this project's specific choices.

---

## Release Workflows

How to get code from "PR merged" to "version published" â€” and the many tools that automate each step. There's no single right answer; the ecosystem has a lot of overlapping approaches. These notes capture what I've learned about the options.

### The Release Lifecycle

Every release workflow, regardless of tooling, follows roughly the same steps:

1. **Open a PR** â€” propose changes, get review
2. **Merge** â€” land the change on the default branch
3. **Determine the next version** â€” based on commit messages, labels, or manual input
4. **Update version metadata** â€” `pyproject.toml`, `__version__`, tags
5. **Generate changelog** â€” from commits, PR titles, or conventional commits
6. **Create a release** â€” GitHub Release, Git tag, or both
7. **Publish artifacts** â€” PyPI, container registry, docs site, etc.

The interesting question is: which of these steps are manual, which are automated, and which tools do the work?

### Strategy 1: Fully Manual

The simplest approach â€” you do everything by hand.

```text
merge PR â†’ edit version in pyproject.toml â†’ git tag â†’ git push --tags â†’ gh release create â†’ twine upload
```

**When it makes sense:** Solo projects, early prototypes, learning how releases work.

**Downsides:** Error-prone, easy to forget a step, version and tag can drift.

### Strategy 2: Version Bump Tools (Semi-Automated)

Use a tool to bump the version, tag, and commit â€” but you trigger it manually.

#### Version Bumping Tools

| Tool | How it works | Version source | Pros | Cons |
|------|-------------|----------------|------|------|
| **hatch version** | `hatch version minor` bumps in pyproject.toml | `[project] version` or `[tool.hatch.version]` | Integrated with Hatch, supports dynamic versioning | Requires Hatch |
| **bump2version / bump-my-version** | Reads `.bumpversion.cfg` or `pyproject.toml`, updates version strings across multiple files | Any file with version strings | Multi-file support, regex-based find/replace | Extra config file (or `[tool.bumpversion]`), can be fiddly |
| **tbump** | `tbump 1.2.3` updates version, commits, tags, pushes | `[tool.tbump]` in pyproject.toml | Single command does commit+tag+push, regex-based | Must pass the exact version (no `major`/`minor` keywords) |
| **setuptools-scm** | Derives version from Git tags at build time â€” no version in source | Git tags | Zero maintenance, always matches Git | Harder to reason about, import-time overhead, needs `[tool.setuptools_scm]` |
| **versioningit** | Like setuptools-scm but for other backends | Git tags + configurable format | Backend-agnostic, flexible format strings | More config than setuptools-scm |
| **hatch-vcs** | Hatchling plugin that reads version from VCS (Git tags) | Git tags via `[tool.hatch.version]` | Integrates with Hatchling builds | Requires Hatch ecosystem |
| **incremental** | Twisted project's versioning tool | `_version.py` file | Used by Twisted/large projects | Less popular outside that ecosystem |
| **dunamai** | Library + CLI for dynamic versions from VCS | Git/Mercurial tags | Language-agnostic, composable with other tools | CLI-only or library â€” not a full release tool |
| **poetry version** | `poetry version minor` | `[tool.poetry] version` | Integrated into Poetry workflow | Poetry-only |
| **pdm bump** | `pdm bump minor` | `[project] version` | Integrated into PDM | PDM-only |

#### Typical semi-automated workflow

```bash
# 1. Bump version (updates pyproject.toml, commits, tags)
hatch version minor
# or: bump-my-version bump minor
# or: tbump 1.3.0

# 2. Push tag to trigger CI
git push origin main --tags

# 3. CI handles the rest (build, publish, release notes)
```

### Strategy 3: Conventional Commits + Automated Release

This is the "commit message is the API" approach. The version bump and changelog are **derived from commit messages** â€” no manual version decisions.

#### How conventional commits drive releases

```text
feat: add user export endpoint    â†’  minor bump (0.2.0 â†’ 0.3.0)
fix: handle null email in signup  â†’  patch bump (0.3.0 â†’ 0.3.1)
feat!: redesign auth API          â†’  major bump (0.3.1 â†’ 1.0.0)
  (or: BREAKING CHANGE: in body)
chore: update CI config           â†’  no release
docs: fix typo in README          â†’  no release
```

#### Tools that consume conventional commits

| Tool | Language | What it does | Outputs | Pros | Cons |
|------|----------|-------------|---------|------|------|
| **python-semantic-release** | Python | Parses commits, bumps version, updates changelog, creates GitHub Release, publishes to PyPI | Version bump, CHANGELOG.md, GitHub Release, PyPI publish | Full pipeline for Python, GitHub Actions friendly | Config can be complex, opinionated defaults |
| **semantic-release** (JS) | Node.js | The original â€” parses commits, bumps, publishes, releases | Version bump, changelog, npm publish, GitHub Release | Massive plugin ecosystem, very mature | Node dependency in a Python project |
| **release-please** (Google) | GitHub Action | Creates a "Release PR" that tracks pending changes; merging the PR triggers the release | Release PR, version bump, CHANGELOG.md, GitHub Release | No local tooling needed, PR-based review of release, monorepo support | Google-maintained (bus factor), opinionated PR flow |
| **commitizen** | Python | Commit message prompting (`cz commit`), version bump, changelog generation | Guided commits, version bump, CHANGELOG.md | Interactive commit helper + release tool in one, Python native | Two jobs in one tool â€” some prefer separation |
| **standard-version** | Node.js | Bump version, generate changelog from conventional commits, tag | Version bump, CHANGELOG.md, Git tag | Simple, focused | Deprecated in favour of release-please |
| **cocogitto** | Rust | Validate conventional commits, bump version, generate changelog | Version bump, CHANGELOG.md, Git tag | Fast, strict validation, good CI integration | Rust binary, smaller community |
| **git-cliff** | Rust | Highly configurable changelog generator (not a full release tool) | CHANGELOG.md | Extremely customisable templates, fast, any commit convention | Changelog only â€” doesn't bump versions or create releases |
| **auto** (Intuit) | Node.js | Label-based releases â€” uses PR labels instead of commit messages | Version bump, changelog, GitHub Release, npm publish | PR-label approach is more accessible than commit conventions | Node dependency, label-driven (different paradigm) |
| **changelogithub** | Node.js | Generate changelog from GitHub PR titles/commits | Changelog, GitHub Release body | Uses GitHub API, pretty output | Changelog only, Node dependency |

### Strategy 4: Release PR Pattern (release-please Style)

This is a higher-level pattern where the tool **opens a PR** that represents the next release, and **merging that PR** triggers the actual release.

#### How it works

```text
1. Contributors merge feature PRs into main
2. Bot watches main, accumulates changes, opens/updates a "Release PR"
3. Release PR contains:
   - Version bump in pyproject.toml (or package.json, etc.)
   - Updated CHANGELOG.md with all changes since last release
4. Maintainer reviews the Release PR
5. Merging the Release PR triggers:
   - Git tag creation
   - GitHub Release creation
   - CI publish workflow (PyPI, npm, etc.)
```

#### Tools supporting the Release PR pattern

| Tool | How the Release PR works | Monorepo | Multi-language |
|------|-------------------------|----------|----------------|
| **release-please** | GitHub Action watches pushes to main, opens/updates a Release PR automatically | Yes (workspace plugins) | Yes (Python, Node, Java, Go, Rust, etc.) |
| **changesets** | CLI generates "changeset" files in PRs; a bot opens a "Version Packages" PR that combines them | Yes (native) | Mainly JS/TS but adaptable |
| **knope** | Rust-based, uses changeset files or conventional commits to generate a Release PR | Yes | Yes (any language) |

### Strategy 5: Tag-Driven Releases (CI Does Everything)

Push a Git tag â†’ CI builds, publishes, releases. The simplest CI-driven approach.

```yaml
# .github/workflows/publish.yml
on:
  push:
    tags: ["v*"]

jobs:
  publish:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@...
      - run: python -m build
      - uses: pypa/gh-action-pypi-publish@...
```

You manually (or via a bump tool) create the tag. CI handles the rest.

### Changelog Generation â€” Deeper Dive

Changelogs can be generated from multiple sources. The tools differ in what they consume and how customisable the output is.

#### Changelog Source Material

| Source | Tools that use it | Pros | Cons |
|--------|------------------|------|------|
| **Conventional commit messages** | python-semantic-release, commitizen, cocogitto, standard-version | Automated, structured, links to commits | Requires discipline from all contributors |
| **PR titles / PR bodies** | release-please, auto, changelogithub | Easier for contributors (just write good PR titles) | Less granular than per-commit |
| **PR labels** | auto (Intuit), release-drafter | Visual, easy to apply retroactively | Extra manual step (labelling), labels can be forgotten |
| **Changeset files** | changesets, knope, towncrier | Each PR includes a human-written changelog fragment | Extra file per PR, merge conflicts possible |
| **Git log (any format)** | git-cliff, gitmoji-changelog | Works with any commit format | Noisy unless commits are clean |
| **Manual** | Keep a Changelog format | Full control, human-quality writing | Easy to forget, drifts from actual changes |

#### Changelog Fragment / Towncrier Pattern

Some projects use **changelog fragments** â€” small files added per-PR that are combined at release time.

| Tool | Fragment format | How it works | Pros | Cons |
|------|----------------|-------------|------|------|
| **towncrier** | `changes/123.feature.md` | Each PR adds a fragment file; `towncrier build` combines them into CHANGELOG | Human-written entries, categorised | Extra file per change, merge conflicts on the directory |
| **changesets** | `.changeset/cool-feature.md` | CLI generates a changeset file; bot combines on release | Interactive CLI, monorepo support | JS-ecosystem origin |
| **knope** | `.changeset/*.md` | Similar to changesets but Rust-based | Cross-language, fast | Newer tool |
| **scriv** | `changelog.d/*.md` | Fragment-based, configurable, Python-native | Flexible templates, Python-friendly | Smaller community |

### PR Automation Tools

Tools that help manage the PR lifecycle itself â€” auto-labelling, auto-merge, auto-assign, etc.

| Tool | What it does | How it works |
|------|-------------|-------------|
| **release-drafter** | Drafts GitHub Release notes from PR labels; auto-labels PRs based on file paths | GitHub Action, reads `.github/release-drafter.yml` |
| **auto-approve** | Auto-approves PRs from trusted bots (Dependabot, Renovate) | GitHub Action with conditions |
| **mergify** | Auto-merge, priority queues, auto-label, CI retries | SaaS with `.mergify.yml` config |
| **kodiak** | Auto-merge when checks pass and PR is approved | GitHub App with `.kodiak.toml` |
| **bulldozer** | Auto-merge + auto-delete branch after merge | GitHub App by Palantir |
| **probot-auto-merge** | Auto-merge based on labels and check status | GitHub App (Probot framework) |
| **actions/labeler** | Auto-label PRs based on changed file paths | GitHub Action with `.github/labeler.yml` |
| **action-automatic-releases** | Create GitHub Releases automatically on tag push | GitHub Action |
| **pr-agent** (CodiumAI) | AI-powered PR review, auto-describe, auto-label | GitHub App or Action |
| **danger-js / danger-python** | Programmable PR review rules (check PR size, missing tests, etc.) | CI step, reads `Dangerfile` |

### Dependency Update Bots

These open PRs to keep dependencies current â€” relevant because they feed into the release pipeline.

| Tool | What it updates | How it works | Pros | Cons |
|------|----------------|-------------|------|------|
| **Dependabot** | pip, npm, GitHub Actions, Docker, Bundler, etc. | GitHub-native, `.github/dependabot.yml` | Zero setup, built into GitHub | Limited grouping, no lock file merging strategy |
| **Renovate** | 50+ package managers | Self-hosted or Mend.io App, `renovate.json` | Extremely configurable, auto-merge rules, grouping, scheduling | Complex config, can be noisy |
| **pyup** | Python (pip, pipenv, poetry) | GitHub App or CLI | Python-focused, safety DB integration | Smaller scope than Renovate |
| **depfu** | npm, Yarn, Bundler | GitHub App | Clean PRs, grouped updates | Limited language support |

### Putting It All Together â€” Example Workflows

#### Minimal (solo project, tag-driven)

```text
1. Work on main
2. hatch version patch â†’ commits + tags
3. git push --tags
4. CI publishes to PyPI on tag push
```

**Tools:** Hatch, GitHub Actions, pypa/gh-action-pypi-publish

#### Mid-size (team, conventional commits)

```text
1. Feature PR â†’ conventional commit messages enforced by commitizen/pre-commit
2. Merge PR to main
3. python-semantic-release in CI:
   - Parses new commits since last tag
   - Bumps version in pyproject.toml
   - Updates CHANGELOG.md
   - Creates Git tag + GitHub Release
   - Publishes to PyPI
```

**Tools:** commitizen (commit helper), python-semantic-release (CI), GitHub Actions

#### Large / monorepo (Release PR pattern)

```text
1. Feature PRs merged to main
2. release-please Action opens/updates a Release PR:
   - Bumps version
   - Updates CHANGELOG.md
   - Lists all changes since last release
3. Maintainer reviews and merges the Release PR
4. Merge triggers: tag â†’ GitHub Release â†’ CI publish
```

**Tools:** release-please, GitHub Actions, pypa/gh-action-pypi-publish

#### Fragment-based (human-written changelogs)

```text
1. Each feature PR includes a changelog fragment (changes/123.feature.md)
2. At release time: towncrier build â†’ combines fragments into CHANGELOG.md
3. bump-my-version bump minor â†’ updates version, commits, tags
4. git push --tags â†’ CI publishes
```

**Tools:** towncrier, bump-my-version, GitHub Actions

### Version Numbering Schemes

Not all projects use SemVer. Here are the common schemes and which tools support them.

| Scheme | Format | When to use | Tools that support it |
|--------|--------|------------|----------------------|
| **SemVer** | `MAJOR.MINOR.PATCH` | Libraries, APIs, anything with a public contract | All of the above |
| **CalVer** | `YYYY.MM.DD` or `YY.MM.MICRO` | Applications, data pipelines, things without API stability promises | bump-my-version, hatch-calver, commitizen (custom), setuptools-scm |
| **PEP 440** | `1.2.3`, `1.2.3.dev4`, `1.2.3a1`, `1.2.3rc1` | Python packages (required for PyPI) | All Python tools enforce this |
| **ZeroVer** | `0.x.y` forever | Projects that never commit to stability (half-joking) | Any tool â€” just never bump major |

### What This Project Uses

This project uses a fully automated release pipeline:

- **Conventional commits** validated by commitizen (pre-commit hook + CI)
- **release-please** for automated Release PRs, CHANGELOG, tags, and GitHub Releases
- **hatch-vcs** for deriving package version from git tags at build time
- **Rebase+merge** strategy for linear history with fine-grained CHANGELOG entries
- **GitHub Actions** for build, publish, SBOM generation on tag push

See [ADR 021](../adr/021-automated-release-pipeline.md) and [releasing.md](../releasing.md) for the full workflow.

---

## Merge Strategies for Integrating into Main

When changes from a feature branch need to get into `main`, there are several strategies. Each produces a different commit history shape, affects traceability, and has implications for tools like `git bisect`, changelog generation, and `git log` readability.

### Direct Push to Main (No Branch, No PR)

The simplest approach: commit directly on `main` and push.

```
main:  A â”€ B â”€ C â”€ D
                     â†‘ your commits land here
```

- **History:** Linear
- **When to use:** Solo projects, trivial changes, CI-only repos
- **Pros:** No overhead, no branches to manage
- **Cons:** No review, no CI checks before landing, no PR record, dangerous for teams
- **Who uses this:** Very small projects, personal repos, config-only repos

### Merge Commit (GitHub Default)

Creates a special commit with **two parents** â€” one from main, one from the branch tip. Preserves the branch topology.

```
main:    A â”€ B â”€ â”€ â”€ â”€ â”€ M
              \         /
feature:       C â”€ D â”€ E
```

Where `M` is the merge commit with parents `B` and `E`.

- **History:** Non-linear (graph shape, "railroad tracks" in `git log --graph`)
- **Original SHAs:** Preserved â€” the branch commits keep their hashes
- **Merge event:** Visible â€” the merge commit marks exactly where the branch was integrated
- **Pros:**
  - Full history preserved with branch context
  - Easy to revert an entire feature: `git revert -m 1 <merge-commit>`
  - Original SHAs intact â€” links to branch commits never break
  - `git log --merges` shows all integration points
- **Cons:**
  - Cluttered history with merge commits between every PR
  - Hard to `git bisect` when merge commits are involved
  - `git log` without `--graph` is confusing (interleaved commits from multiple branches)
  - Non-linear history is harder for tools to parse

### Squash and Merge

Takes all commits from the feature branch and **squashes** them into a **single commit** on main. The PR title typically becomes the commit message.

```
feature:  C â”€ D â”€ E    (3 commits)
                â†“ squash
main:     A â”€ B â”€ S    (1 commit, S = squashed C+D+E)
```

- **History:** Linear (one commit per PR)
- **Original SHAs:** Lost â€” all branch commits are discarded, replaced by one new commit
- **Merge event:** No â€” just a single commit, no visual merge point
- **Pros:**
  - Clean, linear history â€” one commit per logical change
  - PR title becomes the commit message â€” only enforce PR title format
  - Easy to `git bisect` (each commit is one PR's worth of change)
  - Good for messy branches with WIP/fixup commits
- **Cons:**
  - **Loses individual commit detail** â€” can't go back to specific changes within a PR
  - Author attribution may be lost (shows merger, not individual committers via co-authored-by)
  - Can't cherry-pick individual changes from a squashed PR
  - Large PRs become one giant commit â€” hard to review in `git log`

### Rebase and Merge (What This Project Uses)

Takes each commit from the feature branch and **replays** them one at a time on top of main's tip. Produces a linear history where every commit is preserved.

```
feature:  C â”€ D â”€ E           (on top of B)
                  â†“ rebase onto main's tip
main:     A â”€ B â”€ C' â”€ D' â”€ E'  (C', D', E' are replayed copies)
```

The `'` marks indicate new SHAs â€” the commits are re-hashed because their parent changed.

- **History:** Linear (every commit preserved)
- **Original SHAs:** Changed â€” rebased commits get new hashes
- **Merge event:** No â€” no visual merge point in the graph
- **Pros:**
  - **Linear AND detailed** â€” best of both worlds
  - Individual commits preserved â€” can navigate to specific changes
  - Easy to `git bisect` â€” each commit is atomic and testable
  - Clean `git log` â€” no merge commit noise
  - Commit authors preserved â€” individual attribution maintained
  - Fine-grained CHANGELOG â€” tools can generate one entry per commit
- **Cons:**
  - **Original SHAs change** â€” links to branch commits break after rebase
  - **No merge graph** â€” can't see where a PR started/ended in `git log --graph`
  - Requires commit message discipline â€” every commit message matters
  - Force-push needed to update branch after rebase: `git push --force-with-lease`
  - Contributors must understand rebase workflow

### Comparison Summary

| | Merge Commit | Squash+Merge | Rebase+Merge | Direct Push |
|---|---|---|---|---|
| History shape | Graph | Linear | Linear | Linear |
| Commits on main per PR | All + merge | 1 | All | All |
| Original SHAs | Kept | Lost | Changed | Kept |
| Revert entire PR | Easy (`revert -m 1`) | Easy (one commit) | Hard (revert each commit) | N/A |
| `git bisect` | Awkward with merges | Good (coarse) | Good (fine) | Good |
| CHANGELOG granularity | Per commit | Per PR | Per commit | Per commit |
| Commit message enforcement | Per commit | PR title only | Per commit | Per commit |
| Merge event visible | Yes | No | No | N/A |

### Mental Model for Rebase+Merge

Think of rebase as **transplanting** your branch. Your commits are "picked up" from their original base and "replanted" on top of main's latest. The content is the same but the commit IDs change because the parent changed.

The **PR is the integration record** (review comments, approvals, design discussion). The **commit history is the technical audit trail** (what changed, in what order). Together they provide full traceability even without merge commits.

### Branching off Someone Else's Branch (The "Stacked Branch" Problem)

A common scenario: Alice creates `feature-a` off `main`. Bob needs Alice's work, so he branches `feature-b` off `feature-a`. Alice's branch eventually gets merged into `main`. Now Bob's branch has problems.

```
main:       A â”€ B â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ (Alice's work arrives here somehow)
                 \
feature-a:        C â”€ D â”€ E         (Alice's branch)
                          \
feature-b:                 F â”€ G    (Bob's branch, based on Alice's)
```

The **severity of the problem depends on the merge strategy** used to integrate Alice's branch into main.

#### With Merge Commits

When `feature-a` is merged into main with a merge commit:

```
main:       A â”€ B â”€ â”€ â”€ â”€ â”€ M      (M = merge commit, parents: B and E)
                 \         /
feature-a:        C â”€ D â”€ E
                          \
feature-b:                 F â”€ G
```

**Problem:** Mild. Commits C, D, E still exist with their **original SHAs**. Bob's branch is based on E, which is still reachable from main (through the merge). When Bob opens a PR for `feature-b`, git sees that C, D, E are already in main (via M), so the PR diff only shows F and G. **This usually works fine.**

**Potential issue:** If Bob rebases `feature-b` onto `main`, git may get confused about which commits are already applied. The merge commit's two-parent structure can cause unexpected conflicts during rebase.

**Solution:**
```bash
# Bob rebases onto main, skipping Alice's already-merged commits
git checkout feature-b
git rebase --onto main feature-a    # "move F,G from feature-a base to main base"
```

The `--onto` flag says: "take the commits that are on `feature-b` but NOT on `feature-a`, and replay them on top of `main`."

#### With Squash+Merge

When `feature-a` is squash-merged into main:

```
main:       A â”€ B â”€ S              (S = squashed C+D+E into one commit, NEW SHA)
                 \
feature-a:        C â”€ D â”€ E        (these commits are now abandoned)
                          \
feature-b:                 F â”€ G   (still based on E, which is NOT on main)
```

**Problem:** Serious. The squash created a **brand-new commit S** with a different SHA than C, D, or E. Git does NOT know that S contains the same changes as C+D+E. From git's perspective, commits C, D, E are **not on main** â€” only S is. So when Bob tries to rebase or merge `feature-b` onto main:

1. Git tries to replay C, D, E, F, G on top of S
2. C, D, E conflict with S (same changes, different commits)
3. Bob has to manually resolve conflicts for work that's already merged

This is the **most dangerous** strategy for stacked branches.

**Solutions:**

```bash
# Option 1: rebase --onto (skip Alice's commits entirely)
git checkout feature-b
git rebase --onto main feature-a
# This says: "replay only F,G (not C,D,E) onto main"

# Option 2: Interactive rebase â€” drop Alice's commits manually
git checkout feature-b
git rebase -i main
# In the editor, DELETE the lines for commits C, D, E
# Keep only F and G
```

**Prevention:** When you know squash+merge is the strategy, avoid branching off other people's branches. Instead, wait for their PR to be merged, then branch off `main`.

#### With Rebase+Merge

When `feature-a` is rebase-merged into main:

```
main:       A â”€ B â”€ C' â”€ D' â”€ E'    (C', D', E' = rebased copies, NEW SHAs)
                 \
feature-a:        C â”€ D â”€ E          (original SHAs, now orphaned)
                          \
feature-b:                 F â”€ G     (based on E, not E')
```

**Problem:** Moderate. Similar to squash but less severe. The commits C', D', E' on main have **different SHAs** than the originals C, D, E. Git doesn't know they're the same changes. When Bob rebases `feature-b` onto main, git will try to replay C, D, E, F, G and conflict on the duplicated commits.

However, git is often **smarter about this than with squash** because the individual commit patches are identical (same diff, same message). Git's `rebase` command has a built-in mechanism (`--reapply-cherry-picks=false`, which is the default) that can detect "this patch is already applied" and skip it automatically. So sometimes it **just works** â€” but not always, especially if there were conflict resolutions during the original rebase.

**Solutions:**

```bash
# Option 1: rebase --onto (most reliable)
git checkout feature-b
git rebase --onto main feature-a
# Replays only F,G onto main, skipping C,D,E entirely

# Option 2: Plain rebase (often works due to patch-id detection)
git checkout feature-b
git rebase main
# Git may auto-skip C,D,E if it detects matching patches
# But if there were conflicts in the original rebase, this may fail

# Option 3: If plain rebase gives conflicts, abort and use --onto
git rebase --abort
git rebase --onto main feature-a
```

#### Summary: Stacked Branch Risk by Strategy

| Strategy | Risk level | Why | Best fix |
|----------|-----------|-----|----------|
| **Merge commit** | Low | Original SHAs preserved, git knows they're on main | `git rebase --onto main feature-a` if needed |
| **Squash+merge** | High | All original commits replaced with one new SHA, git can't detect duplicates | `git rebase --onto main feature-a` (mandatory) |
| **Rebase+merge** | Medium | New SHAs but identical patches â€” git can often auto-detect | `git rebase main` (try first), fall back to `--onto` |

#### The Universal Fix: `git rebase --onto`

No matter the merge strategy, `git rebase --onto` is the universal fix for stacked branches:

```bash
git rebase --onto <new-base> <old-base> [<branch>]
```

Read it as: "Take commits that are on `<branch>` but NOT on `<old-base>`, and replay them onto `<new-base>`."

```bash
# The pattern is always:
git checkout feature-b
git rebase --onto main feature-a

# Which means:
# "Take commits on feature-b that aren't on feature-a (= F, G)
#  and replay them onto main"
```

**Tip:** If you've already deleted the `feature-a` branch and can't reference it, you can use the SHA of the commit where `feature-b` diverged:

```bash
# Find where feature-b branched off feature-a
git log --oneline feature-b
# Identify commit E (last of Alice's commits)
git rebase --onto main <SHA-of-E> feature-b
```

#### Prevention Strategies

1. **Don't stack** unless necessary â€” wait for the base PR to merge, then branch off `main`
2. **Communicate** â€” if you must stack, tell the base branch author so they don't force-push or rebase without warning
3. **Use `--onto` proactively** â€” as soon as the base branch is merged, immediately rebase your branch with `--onto main`
4. **Keep stacked branches small** â€” the fewer commits, the easier to resolve conflicts
5. **Consider draft PRs** â€” open your stacked PR as draft, noting it depends on the base PR

---

## Git Tags

### What Are Tags?

Tags are **named pointers to specific commits** in git. They're like bookmarks â€” a human-readable label permanently attached to a point in history.

```
main:  A â”€ B â”€ C â”€ D â”€ E â”€ F
                   â†‘           â†‘
                v0.1.0      v1.0.0
```

Tags don't move. Unlike branches (which advance with each new commit), a tag stays put.

### Types of Tags

| Type | Command | What it stores | Use case |
|------|---------|---------------|----------|
| **Lightweight** | `git tag v1.0.0` | Just a pointer to a commit (like a branch that never moves) | Quick labels, local markers |
| **Annotated** | `git tag -a v1.0.0 -m "Release 1.0.0"` | Full git object: tagger name, email, date, message, optional GPG signature | Releases (preferred for public tags) |

### Where Do Tags Live?

| Location | Path | How to see |
|----------|------|-----------|
| **Local** | `.git/refs/tags/` (one file per tag, containing the SHA) | `git tag` or `git tag -l "v1.*"` |
| **Remote** | `refs/tags/` on the remote server | `git ls-remote --tags origin` |

**Important:** Tags are **NOT pushed by default.** You must explicitly push them:

```bash
git push origin v1.0.0          # Push a specific tag
git push origin --tags          # Push ALL local tags
```

### Common Tag Operations

```bash
# List all tags
git tag

# List tags matching a pattern
git tag -l "v1.*"

# Create a lightweight tag
git tag v1.0.0

# Create an annotated tag (preferred for releases)
git tag -a v1.0.0 -m "Release 1.0.0"

# Tag a specific commit (not HEAD)
git tag -a v1.0.0 abc1234 -m "Release 1.0.0"

# Show tag details
git show v1.0.0

# Delete a local tag
git tag -d v1.0.0

# Delete a remote tag
git push origin :refs/tags/v1.0.0
# or
git push origin --delete v1.0.0

# See what commit a tag points to
git rev-parse v1.0.0
```

### How Tags Are Used in This Project

- **release-please** creates annotated tags (e.g., `v1.2.0`) when the Release PR is merged
- **hatch-vcs** reads the latest tag to derive the Python package version at build time
- **release.yml** triggers on `push: tags: v*.*.*` â€” building and publishing on tag creation
- **Convention:** Tags use the `v` prefix (e.g., `v1.0.0`) per SemVer convention

### Tags vs Branches

| | Tags | Branches |
|---|---|---|
| Moves? | No â€” fixed to one commit | Yes â€” advances with each new commit |
| Purpose | Mark a point in time (release, milestone) | Track ongoing work |
| Storage | `.git/refs/tags/` | `.git/refs/heads/` |
| Auto-pushed? | No â€” must explicitly push | Yes â€” with `git push` |

---

## Commit Traceability and PR Linkage

With rebase+merge, individual commits lose their branch context â€” there's no merge commit to mark where a PR started and ended. This raises the question: **how do you trace a commit back to the PR and discussion that produced it?**

### Option A: Let GitHub Handle It Automatically (Recommended)

GitHub's rebase+merge automatically appends `(#PR)` to each commit's subject line when you merge via the web UI. No configuration needed.

**The flow:**

1. You write locally: `feat: add user authentication`
2. You push your branch and open PR #42
3. When you click "Rebase and merge" on GitHub, each commit becomes: `feat: add user authentication (#42)`
4. release-please reads commits on `main` and generates CHANGELOG entries with the `(#42)` link
5. In the rendered CHANGELOG on GitHub, `#42` is automatically a clickable link to the PR

**What the CHANGELOG looks like:**

```markdown
### Features

* add user authentication (#42)
* add login CLI command (#42)
* add password hashing utility (#43)
```

Each `(#42)` links to the full PR with review comments, approvals, and design discussion.

**Why this works well:**
- Zero friction â€” you don't think about it locally, the linkage just appears on `main`
- Commitizen already accepts the `(#PR)` suffix â€” the commit-msg hook won't reject these
- The PR number is always correct (GitHub appends it, not a human)
- Works with every PR, no exceptions

**Important:** This only happens when you merge **via the GitHub UI** (or API). If you push directly to `main` or use `git rebase` locally and push, there's no PR to reference.

### Option B: Require Issue References in Commit Messages

If you want commits to reference an **issue** (not just the PR), you can enforce a pattern in the commit body. This is useful when you want traceability to requirements/tickets, not just PRs.

**Example commit with issue reference:**

```
feat: add user authentication

Refs: #15
```

Or using GitHub's closing keywords:

```
fix: correct token expiration calculation

Fixes #28
```

**How to enforce this with commitizen:**

You can customize the commitizen schema in `pyproject.toml` to require a footer. However, commitizen's built-in `cz_conventional_commits` schema prompts for an optional footer during `cz commit` â€” it just doesn't **require** it.

To strictly enforce issue references, you'd need a custom commitizen plugin or a CI check:

```yaml
# In commit-lint.yml â€” add a step after the cz check:
- name: Check for issue references
  run: |
    # Check that every feat/fix commit has a "Refs:" or "Fixes" footer
    git log --format="%H %s%n%b---" origin/${{ github.base_ref }}..HEAD | \
    awk '
      /^[a-f0-9]+ (feat|fix)/ { needs_ref=1; sha=$1; subject=$0 }
      /Refs:|Fixes|Closes|Resolves/ { needs_ref=0 }
      /^---$/ { if (needs_ref) print "Missing issue ref: " subject; needs_ref=0 }
    '
```

**Trade-offs:**
- More friction â€” developers must know the issue number before committing
- Not all commits map neatly to one issue
- PR linkage (Option A) is already automatic and often sufficient
- Useful for projects with strict requirements traceability (e.g., regulated industries)

### Combining Both Options

You can use both: GitHub auto-appends `(#PR)` and you optionally include `Refs: #issue` in the body. The commit on `main` would look like:

```
feat: add user authentication (#42)

Refs: #15
```

This gives three layers of traceability:
1. **Commit message** â€” what changed and why
2. **`(#42)`** â€” links to the PR (review, discussion, approval)
3. **`Refs: #15`** â€” links to the issue (requirements, user story, bug report)

### Configuring GitHub's Auto-Append Behavior

GitHub's `(#PR)` append behavior is controlled at the repository level:

**Settings â†’ General â†’ Pull Requests â†’ "Pull Request default commit message"**

For each merge strategy, you can choose what GitHub puts in the default commit message:
- **Default message** â€” uses the commit message as-is, appends `(#PR)`
- **Pull request title** â€” uses the PR title as the commit subject
- **Pull request title and description** â€” uses the PR title and body

For rebase+merge specifically, GitHub preserves each individual commit message and appends `(#PR)` to the subject line. This behavior is built-in for rebase+merge and cannot be disabled â€” the `(#PR)` is always appended.

---

## Programming Jargon

Common programming and development terminology, including informal terms you'll encounter in open-source projects, code reviews, and technical discussions.

### General Development Jargon

| Term | Meaning | Example usage |
|------|---------|--------------|
| **Landing** / **Landing a branch** | Getting your changes merged into the main branch. "Landed" = "merged and now on main." | "I landed my feature branch" = "my PR was merged" |
| **Landing on main** | Same as above â€” emphasizes that the changes arrived at their destination. | "Once this lands on main, we can release" |
| **Ship it** | Approve and merge/deploy. Implies confidence that it's ready. | "LGTM, ship it" (in a PR review) |
| **LGTM** | "Looks Good To Me" â€” approval shorthand in code reviews. | Comment on a PR: "LGTM" |
| **Nit** | A nitpick â€” minor style or preference feedback, not a blocker. | "nit: prefer `snake_case` here" |
| **Bikeshedding** | Spending disproportionate time on trivial decisions (color of the bikeshed). | "Let's not bikeshed the variable name â€” either is fine" |
| **Yak shaving** | A series of nested tasks you must complete before doing the original task. | "I needed to fix the linter to fix the import to fix the test to add the feature" |
| **Rubber ducking** | Explaining a problem out loud (even to an inanimate object) to understand it better. | "I rubber-ducked it and realized the bug was in the loop" |
| **Dogfooding** | Using your own product internally before releasing to users. | "We're dogfooding the new API before v2 launch" |
| **Greenfield** | A brand-new project with no existing code or constraints. | "This is a greenfield project â€” no legacy to worry about" |
| **Brownfield** | Working within an existing codebase with established patterns and constraints. | "It's a brownfield project â€” we have to work around the existing schema" |
| **Tech debt** | Shortcuts or suboptimal code that works now but will cost more to maintain later. | "We're accruing tech debt by skipping tests" |
| **Foot gun** | A feature or API that makes it easy to accidentally cause problems. | "`eval()` is a foot gun â€” too easy to introduce security vulnerabilities" |
| **Escape hatch** | A way to bypass normal rules or abstractions when you need to. | "`--no-verify` is the escape hatch for pre-commit hooks" |
| **Happy path** | The expected, error-free flow through code. | "The happy path works, but we need to handle edge cases" |
| **Sad path** | Error or failure scenarios. | "What happens on the sad path â€” when the API is down?" |
| **Blast radius** | How much is affected if something goes wrong. | "The blast radius of this change is small â€” only affects the CLI" |
| **Upstream** / **Downstream** | Upstream = the original source you forked from or depend on. Downstream = consumers of your code. | "We need to submit the fix upstream" |
| **Vendoring** | Copying a dependency's source code directly into your project instead of installing it. | "We vendored the library to avoid the pip dependency" |
| **Shim** | A thin adapter layer that translates between two interfaces. | "We added a shim to support both the old and new API" |
| **Tombstone** | Code or data that's been logically deleted but physically retained (marked as dead). | "The method is a tombstone â€” it exists but is never called" |

### Git-Specific Jargon

| Term | Meaning | Example usage |
|------|---------|--------------|
| **Trunk** | The main development branch (`main` or `master`). From "trunk-based development." | "We develop on trunk â€” no long-lived feature branches" |
| **HEAD** | The current commit your working directory is on. Usually the tip of a branch. | "HEAD is at abc1234" |
| **Detached HEAD** | When HEAD points to a specific commit, not a branch. Commits here aren't on any branch. | "git checkout v1.0.0 puts you in detached HEAD state" |
| **Fast-forward** | When a branch can be moved forward without creating a merge commit (no divergence). | "Pull with `--ff-only` to ensure a clean fast-forward" |
| **Force-push** | Overwriting remote history. Dangerous on shared branches, normal after rebase. | "After rebasing, force-push with `--force-with-lease`" |
| **Cherry-pick** | Applying a single commit from one branch onto another. | "Cherry-pick the hotfix onto the release branch" |
| **Stash** | Temporarily shelving uncommitted changes. | "Stash your changes, switch branches, then pop them back" |
| **Reflog** | Git's safety net â€” a log of every HEAD position change, even after resets or rebases. | "I lost my commit but found it in `git reflog`" |
| **Porcelain** vs **Plumbing** | Porcelain = user-friendly git commands (`git log`). Plumbing = low-level internals (`git cat-file`). | "For scripts, use plumbing commands â€” they have stable output" |

---

## GitHub Copilot Instructions File

### What is `.github/copilot-instructions.md`?

A Markdown file that GitHub Copilot reads **on every interaction** when
working in a repository. It acts as a persistent briefing â€” project
conventions, tool choices, file layout, review priorities, and things to
ignore. Copilot treats its contents as soft rules: it follows them by
default but the user can override with explicit instructions.

The file lives at `.github/copilot-instructions.md` (this is the
convention that VS Code / GitHub Copilot looks for automatically).

### Why It Matters

Without this file, Copilot starts every conversation from scratch â€” it has
to rediscover your project structure, conventions, and tooling by reading
source files. With it, Copilot arrives pre-briefed and:

- Generates code that matches your conventions (imports, naming, type hints)
- Knows which tools to run and how (Hatch, pytest, Ruff, mypy)
- Avoids suggesting patterns you've already decided against
- Keeps documentation, workflows, and config in sync
- Understands your project layout without exploring every directory

Think of it like onboarding documentation, but for your AI pair programmer.

### How Big Should It Be?

This is the most important practical question. Copilot loads the entire
file into its context window on every interaction. That context window is
shared with: your current file, open files, conversation history, and any
files Copilot reads during the session. A bloated instructions file crowds
out the actual code Copilot needs to reason about.

| Range | Verdict | Notes |
|-------|---------|-------|
| **< 100 lines** | Too thin | Likely missing key conventions. Copilot will guess at things you'd rather it know. |
| **100â€“300 lines** | Good starting point | Covers project overview, conventions, review priorities, and key files. Good for small-to-medium projects. |
| **300â€“500 lines** | Sweet spot for complex projects | Room for workflow tables, commit format, tool inventories, and architecture pointers. This boilerplate sits here (~350 lines). |
| **500â€“800 lines** | Caution zone | Still workable if every section pulls its weight. Audit quarterly â€” remove anything that duplicates what's in dedicated docs. |
| **800+ lines** | Diminishing returns | Context window pressure becomes real. Copilot may miss instructions buried in the noise. Split detail into referenced docs. |

**Rule of thumb:** If you can't skim the whole file in 2 minutes, it's
too long. Prefer linking out to detailed docs (ADRs, architecture.md,
tool-decisions.md) rather than inlining everything.

### Recommended Layout

A well-structured instructions file follows this general pattern:

```markdown
# Copilot Instructions

Guidelines for GitHub Copilot when working in this repository.

<!-- TODO (template users): Customise this file for your project. -->

---

## How This Project Works          â† What the project IS
### Overview                        (1-2 paragraphs)
### Build & Environment             (how to build/run)
### Key Configuration Files         (table of important files)
### CI/CD                           (workflow summary if relevant)

## Working Style                   â† How Copilot should BEHAVE
### Keep Related Files in Sync      (cross-reference rules)
### Leave TODOs for Template Users  (if template repo)
### Provide Feedback and Pushback   (don't be a yes-machine)
### Session Recap                   (end-of-session summary format)

## Review Priorities               â† What to WATCH FOR
### High Priority                   (type hints, tests, security)
### Medium Priority                 (docstrings, error handling)
### Low Priority                    (comments, style)
### General Guidance                (minimal diffs, don't churn)

## Conventions                     â† Project RULES
### Language                        (imports, naming, style)
### Project Structure               (where things go)
### Git & PRs                       (commit format, branch rules)

## Ignore / Don't Flag             â† What to SKIP
                                    (disabled rules, generated files)

## Architecture & Design Refs      â† Where to find DEPTH
                                    (links to ADRs, architecture.md, etc.)

## Common Issues to Catch          â† Known PITFALLS
                                    (src/ layout, mutable defaults, etc.)
```

**Key principles:**

1. **Lead with context** â€” "How This Project Works" goes first because
   Copilot needs to understand the project before it can follow rules.

2. **Behaviour before rules** â€” "Working Style" (how to act) before
   "Conventions" (what to enforce). Copilot's collaboration style
   matters more than import order.

3. **Reference, don't duplicate** â€” Link to `architecture.md`,
   `tool-decisions.md`, and ADRs for detailed reasoning. Keep this file
   as a summary layer.

4. **End with escape hatches** â€” "Ignore / Don't Flag" and "Common Issues"
   are quick-reference sections that prevent false positives.

### What to Include vs. Link Out

The goal is **fast orientation**, not exhaustive documentation.

| Include in instructions file | Link out to separate docs |
|------------------------------|--------------------------|
| Project overview (2-3 sentences) | Full architecture (architecture.md) |
| Tool names and how to run them | Tool comparison reasoning (tool-decisions.md) |
| Convention summary (1-2 lines each) | Detailed ADRs (docs/adr/) |
| Workflow table (name + trigger + purpose) | Individual workflow files |
| Commit message format | Full contributing guide |
| What to ignore (disabled rules) | Ruff/mypy full config (pyproject.toml) |

### Maintenance

The instructions file is only useful if it's accurate. Stale instructions
are worse than no instructions â€” they actively mislead Copilot.

**Keep it current by treating it like code:**

- Update it in the same PR that changes what it describes
- Include it in review checklists ("does this change affect copilot-instructions?")
- Consider adding a Copilot meta-instruction: "If a change affects how
  Copilot should work, update this file as part of the same change"
  (this boilerplate does exactly this)

**Signs it needs a trim:**

- Sections that repeat what's in other docs verbatim
- Tool details for tools you removed months ago
- Rules that Ruff/mypy already enforce automatically
- Overly detailed workflow descriptions (the YAML is the source of truth)

### This Project's Instructions File

This boilerplate's `.github/copilot-instructions.md` is ~350 lines and
covers:

| Section | Purpose | ~Lines |
|---------|---------|--------|
| How This Project Works | Build, hooks, workflows, config | ~130 |
| Working Style | TODOs, sync, pushback, recaps | ~70 |
| Review Priorities | What to check at high/med/low priority | ~30 |
| Conventions | Python, project structure, git, CI | ~50 |
| Ignore / Don't Flag | Disabled rules, generated files | ~10 |
| Architecture & Design Refs | Links to deep docs + ADR table | ~40 |
| Common Issues | Known pitfalls | ~10 |

It sits comfortably in the 300â€“500 sweet spot for a project of this
complexity. The heaviest section is "How This Project Works" â€” which is
justifiable because this project has 24 workflows, 30+ pre-commit hooks,
and multiple environments. For simpler projects, that section could be
much shorter.

---

## Resources

### Python Packaging

- [Python Packaging User Guide](https://packaging.python.org/)
- [Hynek's Testing & Packaging](https://hynek.me/articles/testing-packaging/)
- [Real Python: Python Packaging](https://realpython.com/pypi-publish-python-package/)
- [PyPA Sample Project](https://github.com/pypa/sampleproject)
- [PEP 621 â€“ Project metadata in pyproject.toml](https://peps.python.org/pep-0621/)

### Testing & Quality

- [pytest Documentation](https://docs.pytest.org/)
- [Coverage.py](https://coverage.readthedocs.io/)
- [Hypothesis (property-based testing)](https://hypothesis.readthedocs.io/)

### Linting & Formatting

- [Ruff Documentation](https://docs.astral.sh/ruff/)
- [Mypy Documentation](https://mypy.readthedocs.io/)
- [pre-commit](https://pre-commit.com/)

### CI/CD & GitHub Actions

- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [GitHub Actions Security Hardening](https://docs.github.com/en/actions/security-guides/security-hardening-for-github-actions)
- [Dependabot Documentation](https://docs.github.com/en/code-security/dependabot)

### Project Templates & Best Practices

- [Hypermodern Python](https://cjolowicz.github.io/posts/hypermodern-python-01-setup/)
- [Scientific Python Library Development Guide](https://learn.scientific-python.org/development/)
- [The Hitchhiker's Guide to Python](https://docs.python-guide.org/)

### Security

- [pip-audit](https://github.com/pypa/pip-audit)
- [Bandit](https://bandit.readthedocs.io/)
- [OpenSSF Scorecard](https://securityscorecards.dev/)

### Release & Versioning

- [python-semantic-release](https://python-semantic-release.readthedocs.io/)
- [release-please](https://github.com/googleapis/release-please)
- [commitizen (Python)](https://commitizen-tools.github.io/commitizen/)
- [conventional commits spec](https://www.conventionalcommits.org/)
- [git-cliff](https://git-cliff.org/)
- [towncrier](https://towncrier.readthedocs.io/)
- [bump-my-version](https://github.com/callowayproject/bump-my-version)
- [setuptools-scm](https://setuptools-scm.readthedocs.io/)
- [hatch-vcs](https://github.com/ofek/hatch-vcs)
- [changesets](https://github.com/changesets/changesets)
- [auto (Intuit)](https://intuit.github.io/auto/)
- [release-drafter](https://github.com/release-drafter/release-drafter)
- [Keep a Changelog](https://keepachangelog.com/)
- [SemVer spec](https://semver.org/)
- [CalVer](https://calver.org/)
- [PEP 440 â€“ Version Identification](https://peps.python.org/pep-0440/)
